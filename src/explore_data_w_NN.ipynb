{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa1d1dec-85a8-428b-a95f-3afa2ba20878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee384ec1-90f5-45ee-942d-7344fdb3d637",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d627a22-20ed-4174-aab6-b0d2f5e5d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5b4d4f-0026-41cf-b2c0-a3d22e0a41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/zfs/projects/darc/wolee_edehaan_suzienoh-exploratory-ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6fa271f-1aa3-4b31-baff-469cd6d915c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('Neural Network')\n",
    "logger.setLevel(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c54ac0-f43e-4205-a34d-ee76ab6775fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare global variables\n",
    "global con_list\n",
    "global dum_list\n",
    "global embed_list\n",
    "global deps\n",
    "global header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6beb749f-dbab-45ec-9612-f54db61550bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of continuous variables\n",
    "con_list = ['absacc', 'acc', 'aeavol', 'age', 'agr', 'baspread', 'beta', \n",
    "            'betasq', 'bm', 'bm_ia', 'cash', 'cashdebt', 'cashpr','cfp', \n",
    "            'cfp_ia', 'chatoia', 'chcsho', 'chempia', 'chfeps', 'chinv', \n",
    "            'chmom', 'chnanalyst', 'chpmia', 'chtx', 'cinvest', 'currat', \n",
    "            'depr', 'disp', 'dolvol', 'dy', 'ear', 'egr', 'ep', 'fgr5yr', \n",
    "            'gma', 'grcapx', 'grltnoa', 'herf', 'hire', 'idiovol', 'ill', \n",
    "            'indmom', 'invest', 'lev', 'lgr', 'maxret', 'mom12m', 'mom1m', \n",
    "            'mom36m', 'mom6m', 'ms', 'mve', 'mve_ia', 'nanalyst', 'nincr', \n",
    "            'operprof', 'orgcap', 'pchcapx_ia', 'pchcurrat', 'pchdepr', \n",
    "            'pchgm_pchsale', 'pchquick', 'pchsale_pchinvt', 'pchsale_pchrect', \n",
    "            'pchsale_pchxsga', 'pchsaleinv', 'pctacc', 'pricedelay', 'ps', \n",
    "            'quick', 'rd_mve', 'rd_sale', 'realestate', 'retvol', 'roaq', \n",
    "            'roavol', 'roeq', 'roic', 'rsup', 'salecash', 'saleinv', \n",
    "            'salerec', 'secured', 'sfe', 'sgr', 'sp', 'std_dolvol', \n",
    "            'std_turn', 'stdacc', 'stdcf', 'sue', 'tang', 'tb', 'turn', \n",
    "            'zerotrade']\n",
    "\n",
    "\n",
    "# List of dummy variables\n",
    "dum_list = ['convind', 'divi', 'divo', 'ipo', 'rd', 'securedind', 'sin'] # Categorical variable binary\n",
    "\n",
    "# List of embedding variables\n",
    "embed_list = ['permno']\n",
    "\n",
    "# List of dependent variable\n",
    "deps = con_list + dum_list + embed_list + ['date']\n",
    "\n",
    "\n",
    "# Headers\n",
    "header = ['permno','pyear']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71e677b-814b-42b6-9137-270a8dc05524",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "149cff4b-9c5f-43fe-b566-ea32997cab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path, period):\n",
    "    \n",
    "    \"\"\"\n",
    "    Loads and preprocesses the input data.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): The path to the CSV file to be loaded.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Preprocessed pandas DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = [e.lower() for e in df.columns]\n",
    "    \n",
    "    df['date'] = df['date'].copy()\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')\n",
    "    # df['date'] = df['date'].dt.strftime('%m-%d-%Y')\n",
    "\n",
    "    # Extract year\n",
    "    df['pyear'] = df['date'].dt.year\n",
    "    # Remove months if quarterly, otherwise, monthly, keep all months\n",
    "    if period == 'quarter':\n",
    "        df = df[df['date'].dt.month.isin([1,4,7,10])]\n",
    "\n",
    "    # df.sort_values(['permno','date'], inplace=True)\n",
    "    df.sort_values(['date', 'permno'], inplace=True)\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m')\n",
    "    del df['fpedats']\n",
    "    \n",
    "    print(df[['date', 'permno']].head())\n",
    "    print('-' * 50)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13cf36fd-32cb-4d0c-a9d0-6ad86784d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWinsorizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\"\n",
    "    A custom transformer for Winsorizing numeric data.\n",
    "\n",
    "    Attributes:\n",
    "    lower_percentile (int): The lower percentile for clipping data.\n",
    "    upper_percentile (int): The upper percentile for clipping data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lower_percentile, upper_percentile):\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.lower_bound_ = np.percentile(X, self.lower_percentile)\n",
    "        self.upper_bound_ = np.percentile(X, self.upper_percentile)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_clipped = np.clip(X, self.lower_bound_, self.upper_bound_)\n",
    "        \n",
    "        return X_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c78d2ff7-2147-45c5-848c-18d64d93b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class timePeriodMeanTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\"\n",
    "    A custom transformer for imputing missing data based on time period means.\n",
    "\n",
    "    Attributes:\n",
    "    date_column (str): The column name representing dates.\n",
    "    numeric_columns (list): List of numeric column names for which means are calculated.\n",
    "    period (str): The time period for grouping data, either 'quarter' or 'month'.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, date_column, numeric_columns, period='quarter'):\n",
    "        self.date_column = date_column\n",
    "        self.numeric_columns = numeric_columns\n",
    "        self.period = period\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X[self.date_column] = pd.to_datetime(X[self.date_column])\n",
    "        if self.period == 'quarter':\n",
    "            X['Period'] = X[self.date_column].dt.quarter\n",
    "        elif self.period == 'month':\n",
    "            X['Period'] = X[self.date_column].dt.month\n",
    "        else:\n",
    "            raise ValueError(\"period must be 'quarter' or 'month'\")\n",
    "       \n",
    "       # Calculate and store the means of each numeric column for each time period\n",
    "        self.period_means_ = X.groupby('Period')[self.numeric_columns].mean()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X[self.date_column] = pd.to_datetime(X[self.date_column])\n",
    "        if self.period == 'quarter':\n",
    "            X['Period'] = X[self.date_column].dt.quarter\n",
    "        elif self.period == 'month':\n",
    "            X['Period'] = X[self.date_column].dt.month\n",
    "        \n",
    "        for col in self.numeric_columns:\n",
    "            X[col] = X.apply(lambda row: row[col] if not pd.isna(row[col]) \n",
    "                             else self.period_means_.loc[row['Period'], col], axis=1)\n",
    "        # return X.drop(['Period'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99182d02-1d06-4258-abda-a3566a10dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(con_list, dum_list, lower_percentile, upper_percentile, period):\n",
    "    \n",
    "    \"\"\"\n",
    "    Builds a preprocessing pipeline for both numeric and categorical data.\n",
    "\n",
    "    Args:\n",
    "    con_list (list): List of continuous variable names.\n",
    "    dum_list (list): List of dummy (categorical) variable names.\n",
    "    lower_percentile (float): Lower percentile for winsorization.\n",
    "    upper_percentile (float): Upper percentile for winsorization.\n",
    "    period (string): Period for getting mean values (month vs quarter)\n",
    "\n",
    "    Returns:\n",
    "    Pipeline: A composed preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    numeric_pipeline = Pipeline([\n",
    "        # ('fill_na', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)),\n",
    "        ('winsorizer', CustomWinsorizer(lower_percentile=lower_percentile, upper_percentile=upper_percentile)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('impute_con', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0))\n",
    "    ])\n",
    "\n",
    "    categorical_pipeline = Pipeline([\n",
    "        ('impute_cat', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)),\n",
    "    ])\n",
    "\n",
    "    preprocessing = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_pipeline, con_list),\n",
    "            ('cat', categorical_pipeline, dum_list)\n",
    "        ], remainder='passthrough')\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('Time_period_mean_imputation', timePeriodMeanTransformer('date', con_list, period)),\n",
    "        ('Preprocessing', preprocessing),\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ce9ab76-403d-4b01-a885-f724fe8069b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(con_list, dum_list, embed_list, lower_percentile, upper_percentile, period):\n",
    "    \n",
    "    \"\"\"\n",
    "    Builds a preprocessing pipeline for both numeric and categorical data.\n",
    "\n",
    "    Args:\n",
    "    con_list (list): List of continuous variable names.\n",
    "    dum_list (list): List of dummy (categorical) variable names.\n",
    "    lower_percentile (float): Lower percentile for winsorization.\n",
    "    upper_percentile (float): Upper percentile for winsorization.\n",
    "    period (string): Period for getting mean values (month vs quarter)\n",
    "\n",
    "    Returns:\n",
    "    Pipeline: A composed preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    numeric_pipeline = Pipeline([\n",
    "        # ('fill_na', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)),\n",
    "        ('winsorizer', CustomWinsorizer(lower_percentile=lower_percentile, upper_percentile=upper_percentile)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('impute_con', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0))\n",
    "    ])\n",
    "\n",
    "    categorical_pipeline = Pipeline([\n",
    "        ('impute_cat', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)),\n",
    "    ])\n",
    "    \n",
    "    embed_pipeline = Pipeline([\n",
    "        ('impute_embed', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)),\n",
    "    ])\n",
    "\n",
    "    preprocessing = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_pipeline, con_list),\n",
    "            ('cat', categorical_pipeline, dum_list),\n",
    "            ('embed', embed_pipeline, embed_list),\n",
    "        ], remainder='passthrough')\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('Time_period_mean_imputation', timePeriodMeanTransformer('date', con_list, period)),\n",
    "        ('Preprocessing', preprocessing),\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01d3b469-3062-44af-95f7-80e8af10ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_path = 'Info Processing and Mutual Funds/masterv14.csv'\n",
    "period = 'month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fd7e8f7-df1f-46e3-83bf-a3f800a98de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if period == 'quarter':\n",
    "    target = 'retq'\n",
    "elif period == 'month':\n",
    "    target = 'ret'\n",
    "else:\n",
    "    raise ValueError(\"period must be 'quarter' or 'month'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40b2bc2f-14db-4965-add8-4c651fdc8175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and preprocessing data...\n",
      "\n",
      "      date  permno\n",
      "0  1980-01   10006\n",
      "1  1980-01   10057\n",
      "2  1980-01   10103\n",
      "3  1980-01   10137\n",
      "4  1980-01   10145\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "print('\\nLoading and preprocessing data...\\n')\n",
    "df = load_and_preprocess_data(infile_path, period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b51617d-4f82-4e63-9a01-a5099bfb277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values in the target column and get years 2020 or prior\n",
    "df1 = df.dropna(subset=[target])\n",
    "df1 = df1[df1['pyear'] <= 2020]\n",
    "df1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "759cb5f5-5087-4c66-99a7-76ee89d6d023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>adatadate</th>\n",
       "      <th>fyear</th>\n",
       "      <th>sic2</th>\n",
       "      <th>spi</th>\n",
       "      <th>mve_f</th>\n",
       "      <th>bm</th>\n",
       "      <th>ep</th>\n",
       "      <th>cashpr</th>\n",
       "      <th>...</th>\n",
       "      <th>std_dolvol</th>\n",
       "      <th>std_turn</th>\n",
       "      <th>ill</th>\n",
       "      <th>zerotrade</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>rsq1</th>\n",
       "      <th>pricedelay</th>\n",
       "      <th>idiovol</th>\n",
       "      <th>pyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10006</td>\n",
       "      <td>1010</td>\n",
       "      <td>12/31/1978</td>\n",
       "      <td>1978</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>269.308500</td>\n",
       "      <td>1.180962</td>\n",
       "      <td>0.153022</td>\n",
       "      <td>-32.218678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881844</td>\n",
       "      <td>0.635898</td>\n",
       "      <td>2.565667e-08</td>\n",
       "      <td>1.115306e-07</td>\n",
       "      <td>1.060420</td>\n",
       "      <td>1.124491</td>\n",
       "      <td>0.343408</td>\n",
       "      <td>0.029859</td>\n",
       "      <td>0.025576</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10057</td>\n",
       "      <td>1098</td>\n",
       "      <td>09/30/1978</td>\n",
       "      <td>1978</td>\n",
       "      <td>36</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>97.372000</td>\n",
       "      <td>0.956692</td>\n",
       "      <td>0.135131</td>\n",
       "      <td>-4.408581</td>\n",
       "      <td>...</td>\n",
       "      <td>1.368363</td>\n",
       "      <td>2.546787</td>\n",
       "      <td>2.719812e-07</td>\n",
       "      <td>6.199128e-08</td>\n",
       "      <td>1.526013</td>\n",
       "      <td>2.328716</td>\n",
       "      <td>0.307905</td>\n",
       "      <td>0.092667</td>\n",
       "      <td>0.037473</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10103</td>\n",
       "      <td>1012</td>\n",
       "      <td>10/31/1978</td>\n",
       "      <td>1978</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.697500</td>\n",
       "      <td>3.362003</td>\n",
       "      <td>0.338144</td>\n",
       "      <td>-17.143817</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.759493</td>\n",
       "      <td>3.095816</td>\n",
       "      <td>0.096753</td>\n",
       "      <td>0.221851</td>\n",
       "      <td>0.087020</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10137</td>\n",
       "      <td>1279</td>\n",
       "      <td>12/31/1978</td>\n",
       "      <td>1978</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>537.524500</td>\n",
       "      <td>1.330341</td>\n",
       "      <td>0.153238</td>\n",
       "      <td>-87.819837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553246</td>\n",
       "      <td>0.740017</td>\n",
       "      <td>1.765620e-08</td>\n",
       "      <td>9.726790e-08</td>\n",
       "      <td>0.492885</td>\n",
       "      <td>0.242936</td>\n",
       "      <td>0.189693</td>\n",
       "      <td>0.125777</td>\n",
       "      <td>0.017540</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10145</td>\n",
       "      <td>1300</td>\n",
       "      <td>12/31/1978</td>\n",
       "      <td>1978</td>\n",
       "      <td>99</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>805.633282</td>\n",
       "      <td>1.579284</td>\n",
       "      <td>0.149248</td>\n",
       "      <td>-22.050470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427617</td>\n",
       "      <td>0.657563</td>\n",
       "      <td>2.898901e-09</td>\n",
       "      <td>6.190654e-08</td>\n",
       "      <td>1.139163</td>\n",
       "      <td>1.297691</td>\n",
       "      <td>0.279437</td>\n",
       "      <td>0.024228</td>\n",
       "      <td>0.031201</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165510</th>\n",
       "      <td>93422</td>\n",
       "      <td>154357</td>\n",
       "      <td>12/31/2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>1069.650000</td>\n",
       "      <td>2.487356</td>\n",
       "      <td>-0.090964</td>\n",
       "      <td>-14.117559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797001</td>\n",
       "      <td>12.233361</td>\n",
       "      <td>7.505129e-09</td>\n",
       "      <td>5.571619e-09</td>\n",
       "      <td>2.691027</td>\n",
       "      <td>7.241625</td>\n",
       "      <td>0.265207</td>\n",
       "      <td>0.257939</td>\n",
       "      <td>0.132692</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165511</th>\n",
       "      <td>93423</td>\n",
       "      <td>10567</td>\n",
       "      <td>12/31/2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>79</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>3817.839740</td>\n",
       "      <td>-0.187572</td>\n",
       "      <td>0.046902</td>\n",
       "      <td>19.464647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519033</td>\n",
       "      <td>17.649093</td>\n",
       "      <td>4.462048e-10</td>\n",
       "      <td>3.803709e-09</td>\n",
       "      <td>1.921529</td>\n",
       "      <td>3.692274</td>\n",
       "      <td>0.485215</td>\n",
       "      <td>0.068369</td>\n",
       "      <td>0.061119</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165512</th>\n",
       "      <td>93426</td>\n",
       "      <td>185138</td>\n",
       "      <td>12/31/2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.0108</td>\n",
       "      <td>459.782000</td>\n",
       "      <td>0.524944</td>\n",
       "      <td>0.048258</td>\n",
       "      <td>1.095352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473367</td>\n",
       "      <td>2.144264</td>\n",
       "      <td>2.296462e-08</td>\n",
       "      <td>3.236729e-08</td>\n",
       "      <td>1.302016</td>\n",
       "      <td>1.695247</td>\n",
       "      <td>0.472220</td>\n",
       "      <td>0.037482</td>\n",
       "      <td>0.043174</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165513</th>\n",
       "      <td>93434</td>\n",
       "      <td>184259</td>\n",
       "      <td>06/30/2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1349</td>\n",
       "      <td>87.853920</td>\n",
       "      <td>1.138777</td>\n",
       "      <td>-0.105914</td>\n",
       "      <td>-13.505851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935967</td>\n",
       "      <td>0.897075</td>\n",
       "      <td>3.435272e-07</td>\n",
       "      <td>1.037670e-07</td>\n",
       "      <td>0.389842</td>\n",
       "      <td>0.151977</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>-0.694649</td>\n",
       "      <td>0.073887</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165514</th>\n",
       "      <td>93436</td>\n",
       "      <td>184996</td>\n",
       "      <td>12/31/2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>37</td>\n",
       "      <td>-0.0047</td>\n",
       "      <td>75717.730000</td>\n",
       "      <td>0.087404</td>\n",
       "      <td>-0.011384</td>\n",
       "      <td>8.295322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531632</td>\n",
       "      <td>18.110463</td>\n",
       "      <td>1.820359e-12</td>\n",
       "      <td>2.654462e-09</td>\n",
       "      <td>1.349577</td>\n",
       "      <td>1.821357</td>\n",
       "      <td>0.207462</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.083041</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2165515 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         permno   gvkey   adatadate  fyear  sic2     spi         mve_f  \\\n",
       "0         10006    1010  12/31/1978   1978    37  0.0000    269.308500   \n",
       "1         10057    1098  09/30/1978   1978    36  0.0000     97.372000   \n",
       "2         10103    1012  10/31/1978   1978    33     NaN      1.697500   \n",
       "3         10137    1279  12/31/1978   1978    49     NaN    537.524500   \n",
       "4         10145    1300  12/31/1978   1978    99 -0.0031    805.633282   \n",
       "...         ...     ...         ...    ...   ...     ...           ...   \n",
       "2165510   93422  154357  12/31/2019   2019    13 -0.0090   1069.650000   \n",
       "2165511   93423   10567  12/31/2019   2019    79  0.0004   3817.839740   \n",
       "2165512   93426  185138  12/31/2019   2019    36 -0.0108    459.782000   \n",
       "2165513   93434  184259  06/30/2019   2019     1 -0.1349     87.853920   \n",
       "2165514   93436  184996  12/31/2019   2019    37 -0.0047  75717.730000   \n",
       "\n",
       "               bm        ep     cashpr  ...  std_dolvol   std_turn  \\\n",
       "0        1.180962  0.153022 -32.218678  ...    0.881844   0.635898   \n",
       "1        0.956692  0.135131  -4.408581  ...    1.368363   2.546787   \n",
       "2        3.362003  0.338144 -17.143817  ...         NaN        NaN   \n",
       "3        1.330341  0.153238 -87.819837  ...    0.553246   0.740017   \n",
       "4        1.579284  0.149248 -22.050470  ...    0.427617   0.657563   \n",
       "...           ...       ...        ...  ...         ...        ...   \n",
       "2165510  2.487356 -0.090964 -14.117559  ...    0.797001  12.233361   \n",
       "2165511 -0.187572  0.046902  19.464647  ...    0.519033  17.649093   \n",
       "2165512  0.524944  0.048258   1.095352  ...    0.473367   2.144264   \n",
       "2165513  1.138777 -0.105914 -13.505851  ...    0.935967   0.897075   \n",
       "2165514  0.087404 -0.011384   8.295322  ...    0.531632  18.110463   \n",
       "\n",
       "                  ill     zerotrade      beta    betasq      rsq1  pricedelay  \\\n",
       "0        2.565667e-08  1.115306e-07  1.060420  1.124491  0.343408    0.029859   \n",
       "1        2.719812e-07  6.199128e-08  1.526013  2.328716  0.307905    0.092667   \n",
       "2                 NaN           NaN  1.759493  3.095816  0.096753    0.221851   \n",
       "3        1.765620e-08  9.726790e-08  0.492885  0.242936  0.189693    0.125777   \n",
       "4        2.898901e-09  6.190654e-08  1.139163  1.297691  0.279437    0.024228   \n",
       "...               ...           ...       ...       ...       ...         ...   \n",
       "2165510  7.505129e-09  5.571619e-09  2.691027  7.241625  0.265207    0.257939   \n",
       "2165511  4.462048e-10  3.803709e-09  1.921529  3.692274  0.485215    0.068369   \n",
       "2165512  2.296462e-08  3.236729e-08  1.302016  1.695247  0.472220    0.037482   \n",
       "2165513  3.435272e-07  1.037670e-07  0.389842  0.151977  0.021429   -0.694649   \n",
       "2165514  1.820359e-12  2.654462e-09  1.349577  1.821357  0.207462    0.027211   \n",
       "\n",
       "          idiovol  pyear  \n",
       "0        0.025576   1980  \n",
       "1        0.037473   1980  \n",
       "2        0.087020   1980  \n",
       "3        0.017540   1980  \n",
       "4        0.031201   1980  \n",
       "...           ...    ...  \n",
       "2165510  0.132692   2020  \n",
       "2165511  0.061119   2020  \n",
       "2165512  0.043174   2020  \n",
       "2165513  0.073887   2020  \n",
       "2165514  0.083041   2020  \n",
       "\n",
       "[2165515 rows x 158 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d89ac69-16e9-4e76-825c-b96be297262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "permno_list = list(df1['permno'].sample(1000, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "396cfefd-08e2-44ea-be14-fc0092c8b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df1.loc[df1['permno'].isin(permno_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01b04f7f-4b49-4c5a-a59c-ef16303167a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10698</td>\n",
       "      <td>1980-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10866</td>\n",
       "      <td>1980-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10912</td>\n",
       "      <td>1980-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>11260</td>\n",
       "      <td>1980-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>11308</td>\n",
       "      <td>1980-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165462</th>\n",
       "      <td>93073</td>\n",
       "      <td>2020-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165466</th>\n",
       "      <td>93089</td>\n",
       "      <td>2020-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165472</th>\n",
       "      <td>93130</td>\n",
       "      <td>2020-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165484</th>\n",
       "      <td>93246</td>\n",
       "      <td>2020-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165503</th>\n",
       "      <td>93371</td>\n",
       "      <td>2020-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         permno     date\n",
       "31        10698  1980-01\n",
       "41        10866  1980-01\n",
       "46        10912  1980-01\n",
       "60        11260  1980-01\n",
       "62        11308  1980-01\n",
       "...         ...      ...\n",
       "2165462   93073  2020-12\n",
       "2165466   93089  2020-12\n",
       "2165472   93130  2020-12\n",
       "2165484   93246  2020-12\n",
       "2165503   93371  2020-12\n",
       "\n",
       "[210263 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[['permno', 'date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f42a7b-4be0-4ef0-beb4-60d50bd4f4d2",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b1101b7-ed38-4fc8-a68b-43569e30e8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training in progress...\\n')\n",
    "# Build a training pipeline\n",
    "pipeline = build_pipeline(con_list, dum_list, embed_list, 5, 95, period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa12eebe-1bf5-41cb-9751-0d9ae9dc1f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Time_period_mean_imputation&#x27;,\n",
       "                 timePeriodMeanTransformer(date_column=&#x27;date&#x27;,\n",
       "                                           numeric_columns=[&#x27;absacc&#x27;, &#x27;acc&#x27;,\n",
       "                                                            &#x27;aeavol&#x27;, &#x27;age&#x27;,\n",
       "                                                            &#x27;agr&#x27;, &#x27;baspread&#x27;,\n",
       "                                                            &#x27;beta&#x27;, &#x27;betasq&#x27;,\n",
       "                                                            &#x27;bm&#x27;, &#x27;bm_ia&#x27;,\n",
       "                                                            &#x27;cash&#x27;, &#x27;cashdebt&#x27;,\n",
       "                                                            &#x27;cashpr&#x27;, &#x27;cfp&#x27;,\n",
       "                                                            &#x27;cfp_ia&#x27;, &#x27;chatoia&#x27;,\n",
       "                                                            &#x27;chcsho&#x27;, &#x27;chempia&#x27;,\n",
       "                                                            &#x27;chfeps&#x27;, &#x27;chinv&#x27;,\n",
       "                                                            &#x27;chmom&#x27;,\n",
       "                                                            &#x27;chnanalyst&#x27;,\n",
       "                                                            &#x27;chpmia&#x27;, &#x27;chtx&#x27;,\n",
       "                                                            &#x27;cinvest&#x27;, &#x27;currat&#x27;,\n",
       "                                                            &#x27;depr&#x27;, &#x27;disp&#x27;,\n",
       "                                                            &#x27;dolvol...\n",
       "                                                   &#x27;chempia&#x27;, &#x27;chfeps&#x27;, &#x27;chinv&#x27;,\n",
       "                                                   &#x27;chmom&#x27;, &#x27;chnanalyst&#x27;,\n",
       "                                                   &#x27;chpmia&#x27;, &#x27;chtx&#x27;, &#x27;cinvest&#x27;,\n",
       "                                                   &#x27;currat&#x27;, &#x27;depr&#x27;, &#x27;disp&#x27;,\n",
       "                                                   &#x27;dolvol&#x27;, &#x27;dy&#x27;, ...]),\n",
       "                                                 (&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute_cat&#x27;,\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy=&#x27;constant&#x27;))]),\n",
       "                                                  [&#x27;convind&#x27;, &#x27;divi&#x27;, &#x27;divo&#x27;,\n",
       "                                                   &#x27;ipo&#x27;, &#x27;rd&#x27;, &#x27;securedind&#x27;,\n",
       "                                                   &#x27;sin&#x27;]),\n",
       "                                                 (&#x27;embed&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute_embed&#x27;,\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy=&#x27;constant&#x27;))]),\n",
       "                                                  [&#x27;permno&#x27;])]))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>Pipeline(steps=[(&#x27;Time_period_mean_imputation&#x27;,\n",
       "                 timePeriodMeanTransformer(date_column=&#x27;date&#x27;,\n",
       "                                           numeric_columns=[&#x27;absacc&#x27;, &#x27;acc&#x27;,\n",
       "                                                            &#x27;aeavol&#x27;, &#x27;age&#x27;,\n",
       "                                                            &#x27;agr&#x27;, &#x27;baspread&#x27;,\n",
       "                                                            &#x27;beta&#x27;, &#x27;betasq&#x27;,\n",
       "                                                            &#x27;bm&#x27;, &#x27;bm_ia&#x27;,\n",
       "                                                            &#x27;cash&#x27;, &#x27;cashdebt&#x27;,\n",
       "                                                            &#x27;cashpr&#x27;, &#x27;cfp&#x27;,\n",
       "                                                            &#x27;cfp_ia&#x27;, &#x27;chatoia&#x27;,\n",
       "                                                            &#x27;chcsho&#x27;, &#x27;chempia&#x27;,\n",
       "                                                            &#x27;chfeps&#x27;, &#x27;chinv&#x27;,\n",
       "                                                            &#x27;chmom&#x27;,\n",
       "                                                            &#x27;chnanalyst&#x27;,\n",
       "                                                            &#x27;chpmia&#x27;, &#x27;chtx&#x27;,\n",
       "                                                            &#x27;cinvest&#x27;, &#x27;currat&#x27;,\n",
       "                                                            &#x27;depr&#x27;, &#x27;disp&#x27;,\n",
       "                                                            &#x27;dolvol...\n",
       "                                                   &#x27;chempia&#x27;, &#x27;chfeps&#x27;, &#x27;chinv&#x27;,\n",
       "                                                   &#x27;chmom&#x27;, &#x27;chnanalyst&#x27;,\n",
       "                                                   &#x27;chpmia&#x27;, &#x27;chtx&#x27;, &#x27;cinvest&#x27;,\n",
       "                                                   &#x27;currat&#x27;, &#x27;depr&#x27;, &#x27;disp&#x27;,\n",
       "                                                   &#x27;dolvol&#x27;, &#x27;dy&#x27;, ...]),\n",
       "                                                 (&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute_cat&#x27;,\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy=&#x27;constant&#x27;))]),\n",
       "                                                  [&#x27;convind&#x27;, &#x27;divi&#x27;, &#x27;divo&#x27;,\n",
       "                                                   &#x27;ipo&#x27;, &#x27;rd&#x27;, &#x27;securedind&#x27;,\n",
       "                                                   &#x27;sin&#x27;]),\n",
       "                                                 (&#x27;embed&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute_embed&#x27;,\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy=&#x27;constant&#x27;))]),\n",
       "                                                  [&#x27;permno&#x27;])]))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">timePeriodMeanTransformer</label><div class=\"sk-toggleable__content \"><pre>timePeriodMeanTransformer(date_column=&#x27;date&#x27;,\n",
       "                          numeric_columns=[&#x27;absacc&#x27;, &#x27;acc&#x27;, &#x27;aeavol&#x27;, &#x27;age&#x27;,\n",
       "                                           &#x27;agr&#x27;, &#x27;baspread&#x27;, &#x27;beta&#x27;, &#x27;betasq&#x27;,\n",
       "                                           &#x27;bm&#x27;, &#x27;bm_ia&#x27;, &#x27;cash&#x27;, &#x27;cashdebt&#x27;,\n",
       "                                           &#x27;cashpr&#x27;, &#x27;cfp&#x27;, &#x27;cfp_ia&#x27;, &#x27;chatoia&#x27;,\n",
       "                                           &#x27;chcsho&#x27;, &#x27;chempia&#x27;, &#x27;chfeps&#x27;,\n",
       "                                           &#x27;chinv&#x27;, &#x27;chmom&#x27;, &#x27;chnanalyst&#x27;,\n",
       "                                           &#x27;chpmia&#x27;, &#x27;chtx&#x27;, &#x27;cinvest&#x27;,\n",
       "                                           &#x27;currat&#x27;, &#x27;depr&#x27;, &#x27;disp&#x27;, &#x27;dolvol&#x27;,\n",
       "                                           &#x27;dy&#x27;, ...],\n",
       "                          period=&#x27;month&#x27;)</pre></div> </div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;Preprocessing: ColumnTransformer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for Preprocessing: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content \"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;winsorizer&#x27;,\n",
       "                                                  CustomWinsorizer(lower_percentile=5,\n",
       "                                                                   upper_percentile=95)),\n",
       "                                                 (&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                                 (&#x27;impute_con&#x27;,\n",
       "                                                  SimpleImputer(fill_value=0,\n",
       "                                                                strategy=&#x27;constant&#x27;))]),\n",
       "                                 [&#x27;absacc&#x27;, &#x27;acc&#x27;, &#x27;aeavol&#x27;, &#x27;age&#x27;, &#x27;agr&#x27;,\n",
       "                                  &#x27;baspread&#x27;, &#x27;beta&#x27;, &#x27;betasq&#x27;, &#x27;bm&#x27;, &#x27;bm_ia&#x27;,\n",
       "                                  &#x27;cash&#x27;, &#x27;cashdebt&#x27;, &#x27;cashpr&#x27;, &#x27;c...\n",
       "                                  &#x27;chatoia&#x27;, &#x27;chcsho&#x27;, &#x27;chempia&#x27;, &#x27;chfeps&#x27;,\n",
       "                                  &#x27;chinv&#x27;, &#x27;chmom&#x27;, &#x27;chnanalyst&#x27;, &#x27;chpmia&#x27;,\n",
       "                                  &#x27;chtx&#x27;, &#x27;cinvest&#x27;, &#x27;currat&#x27;, &#x27;depr&#x27;, &#x27;disp&#x27;,\n",
       "                                  &#x27;dolvol&#x27;, &#x27;dy&#x27;, ...]),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;impute_cat&#x27;,\n",
       "                                                  SimpleImputer(fill_value=0,\n",
       "                                                                strategy=&#x27;constant&#x27;))]),\n",
       "                                 [&#x27;convind&#x27;, &#x27;divi&#x27;, &#x27;divo&#x27;, &#x27;ipo&#x27;, &#x27;rd&#x27;,\n",
       "                                  &#x27;securedind&#x27;, &#x27;sin&#x27;]),\n",
       "                                (&#x27;embed&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;impute_embed&#x27;,\n",
       "                                                  SimpleImputer(fill_value=0,\n",
       "                                                                strategy=&#x27;constant&#x27;))]),\n",
       "                                 [&#x27;permno&#x27;])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">num</label><div class=\"sk-toggleable__content \"><pre>[&#x27;absacc&#x27;, &#x27;acc&#x27;, &#x27;aeavol&#x27;, &#x27;age&#x27;, &#x27;agr&#x27;, &#x27;baspread&#x27;, &#x27;beta&#x27;, &#x27;betasq&#x27;, &#x27;bm&#x27;, &#x27;bm_ia&#x27;, &#x27;cash&#x27;, &#x27;cashdebt&#x27;, &#x27;cashpr&#x27;, &#x27;cfp&#x27;, &#x27;cfp_ia&#x27;, &#x27;chatoia&#x27;, &#x27;chcsho&#x27;, &#x27;chempia&#x27;, &#x27;chfeps&#x27;, &#x27;chinv&#x27;, &#x27;chmom&#x27;, &#x27;chnanalyst&#x27;, &#x27;chpmia&#x27;, &#x27;chtx&#x27;, &#x27;cinvest&#x27;, &#x27;currat&#x27;, &#x27;depr&#x27;, &#x27;disp&#x27;, &#x27;dolvol&#x27;, &#x27;dy&#x27;, &#x27;ear&#x27;, &#x27;egr&#x27;, &#x27;ep&#x27;, &#x27;fgr5yr&#x27;, &#x27;gma&#x27;, &#x27;grcapx&#x27;, &#x27;grltnoa&#x27;, &#x27;herf&#x27;, &#x27;hire&#x27;, &#x27;idiovol&#x27;, &#x27;ill&#x27;, &#x27;indmom&#x27;, &#x27;invest&#x27;, &#x27;lev&#x27;, &#x27;lgr&#x27;, &#x27;maxret&#x27;, &#x27;mom12m&#x27;, &#x27;mom1m&#x27;, &#x27;mom36m&#x27;, &#x27;mom6m&#x27;, &#x27;ms&#x27;, &#x27;mve&#x27;, &#x27;mve_ia&#x27;, &#x27;nanalyst&#x27;, &#x27;nincr&#x27;, &#x27;operprof&#x27;, &#x27;orgcap&#x27;, &#x27;pchcapx_ia&#x27;, &#x27;pchcurrat&#x27;, &#x27;pchdepr&#x27;, &#x27;pchgm_pchsale&#x27;, &#x27;pchquick&#x27;, &#x27;pchsale_pchinvt&#x27;, &#x27;pchsale_pchrect&#x27;, &#x27;pchsale_pchxsga&#x27;, &#x27;pchsaleinv&#x27;, &#x27;pctacc&#x27;, &#x27;pricedelay&#x27;, &#x27;ps&#x27;, &#x27;quick&#x27;, &#x27;rd_mve&#x27;, &#x27;rd_sale&#x27;, &#x27;realestate&#x27;, &#x27;retvol&#x27;, &#x27;roaq&#x27;, &#x27;roavol&#x27;, &#x27;roeq&#x27;, &#x27;roic&#x27;, &#x27;rsup&#x27;, &#x27;salecash&#x27;, &#x27;saleinv&#x27;, &#x27;salerec&#x27;, &#x27;secured&#x27;, &#x27;sfe&#x27;, &#x27;sgr&#x27;, &#x27;sp&#x27;, &#x27;std_dolvol&#x27;, &#x27;std_turn&#x27;, &#x27;stdacc&#x27;, &#x27;stdcf&#x27;, &#x27;sue&#x27;, &#x27;tang&#x27;, &#x27;tb&#x27;, &#x27;turn&#x27;, &#x27;zerotrade&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">CustomWinsorizer</label><div class=\"sk-toggleable__content \"><pre>CustomWinsorizer(lower_percentile=5, upper_percentile=95)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content \"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(fill_value=0, strategy=&#x27;constant&#x27;)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">cat</label><div class=\"sk-toggleable__content \"><pre>[&#x27;convind&#x27;, &#x27;divi&#x27;, &#x27;divo&#x27;, &#x27;ipo&#x27;, &#x27;rd&#x27;, &#x27;securedind&#x27;, &#x27;sin&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(fill_value=0, strategy=&#x27;constant&#x27;)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">embed</label><div class=\"sk-toggleable__content \"><pre>[&#x27;permno&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(fill_value=0, strategy=&#x27;constant&#x27;)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">remainder</label><div class=\"sk-toggleable__content \"><pre></pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">passthrough</label><div class=\"sk-toggleable__content \"><pre>passthrough</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Time_period_mean_imputation',\n",
       "                 timePeriodMeanTransformer(date_column='date',\n",
       "                                           numeric_columns=['absacc', 'acc',\n",
       "                                                            'aeavol', 'age',\n",
       "                                                            'agr', 'baspread',\n",
       "                                                            'beta', 'betasq',\n",
       "                                                            'bm', 'bm_ia',\n",
       "                                                            'cash', 'cashdebt',\n",
       "                                                            'cashpr', 'cfp',\n",
       "                                                            'cfp_ia', 'chatoia',\n",
       "                                                            'chcsho', 'chempia',\n",
       "                                                            'chfeps', 'chinv',\n",
       "                                                            'chmom',\n",
       "                                                            'chnanalyst',\n",
       "                                                            'chpmia', 'chtx',\n",
       "                                                            'cinvest', 'currat',\n",
       "                                                            'depr', 'disp',\n",
       "                                                            'dolvol...\n",
       "                                                   'chempia', 'chfeps', 'chinv',\n",
       "                                                   'chmom', 'chnanalyst',\n",
       "                                                   'chpmia', 'chtx', 'cinvest',\n",
       "                                                   'currat', 'depr', 'disp',\n",
       "                                                   'dolvol', 'dy', ...]),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('impute_cat',\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy='constant'))]),\n",
       "                                                  ['convind', 'divi', 'divo',\n",
       "                                                   'ipo', 'rd', 'securedind',\n",
       "                                                   'sin']),\n",
       "                                                 ('embed',\n",
       "                                                  Pipeline(steps=[('impute_embed',\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy='constant'))]),\n",
       "                                                  ['permno'])]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7ee9346-4d3a-48aa-ba40-01876bf22a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set year range of the sample\n",
    "years = list(sample['pyear'].drop_duplicates().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1d745c2-9321-40df-8a7d-c68c8d9c21d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1980,\n",
       " 1981,\n",
       " 1982,\n",
       " 1983,\n",
       " 1984,\n",
       " 1985,\n",
       " 1986,\n",
       " 1987,\n",
       " 1988,\n",
       " 1989,\n",
       " 1990,\n",
       " 1991,\n",
       " 1992,\n",
       " 1993,\n",
       " 1994,\n",
       " 1995,\n",
       " 1996,\n",
       " 1997,\n",
       " 1998,\n",
       " 1999,\n",
       " 2000,\n",
       " 2001,\n",
       " 2002,\n",
       " 2003,\n",
       " 2004,\n",
       " 2005,\n",
       " 2006,\n",
       " 2007,\n",
       " 2008,\n",
       " 2009,\n",
       " 2010,\n",
       " 2011,\n",
       " 2012,\n",
       " 2013,\n",
       " 2014,\n",
       " 2015,\n",
       " 2016,\n",
       " 2017,\n",
       " 2018,\n",
       " 2019,\n",
       " 2020]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a141a7e-aa14-446a-8831-3de0fbbb6df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dee710e1-779f-421e-8d60-836ea53d8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sample.loc[(sample['pyear']<=year)]\n",
    "test_data = sample.loc[(sample['pyear']==year+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "150838b3-3e02-4de7-b239-6faf19fc4bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Years: [1980, 1981, 1982, 1983, 1984, 1985, 1986]\n",
      "\n",
      "Testing Year: [1987]\n"
     ]
    }
   ],
   "source": [
    "# Training and testing data\n",
    "training_years = sorted(train_data.pyear.unique())\n",
    "print(f'Training Years: {training_years}\\n')\n",
    "print(f'Testing Year: {test_data.pyear.unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "418e2bca-401a-475f-a35d-3fe7a41ede7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>adatadate</th>\n",
       "      <th>fyear</th>\n",
       "      <th>sic2</th>\n",
       "      <th>spi</th>\n",
       "      <th>mve_f</th>\n",
       "      <th>bm</th>\n",
       "      <th>ep</th>\n",
       "      <th>cashpr</th>\n",
       "      <th>...</th>\n",
       "      <th>std_dolvol</th>\n",
       "      <th>std_turn</th>\n",
       "      <th>ill</th>\n",
       "      <th>zerotrade</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>rsq1</th>\n",
       "      <th>pricedelay</th>\n",
       "      <th>idiovol</th>\n",
       "      <th>pyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10698</td>\n",
       "      <td>2040</td>\n",
       "      <td>12/31/1978</td>\n",
       "      <td>1978</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>68.839375</td>\n",
       "      <td>1.269332</td>\n",
       "      <td>0.116634</td>\n",
       "      <td>-13.067487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753815</td>\n",
       "      <td>2.894962</td>\n",
       "      <td>4.539289e-08</td>\n",
       "      <td>2.609996e-08</td>\n",
       "      <td>0.589829</td>\n",
       "      <td>0.347898</td>\n",
       "      <td>0.063211</td>\n",
       "      <td>0.037030</td>\n",
       "      <td>0.038551</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10866</td>\n",
       "      <td>2436</td>\n",
       "      <td>10/31/1978</td>\n",
       "      <td>1978</td>\n",
       "      <td>56</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>180.971833</td>\n",
       "      <td>1.357946</td>\n",
       "      <td>0.184896</td>\n",
       "      <td>-6.925387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.759439</td>\n",
       "      <td>0.317240</td>\n",
       "      <td>1.205612e-07</td>\n",
       "      <td>2.628598e-07</td>\n",
       "      <td>0.609418</td>\n",
       "      <td>0.371390</td>\n",
       "      <td>0.122947</td>\n",
       "      <td>-0.228825</td>\n",
       "      <td>0.028520</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10912</td>\n",
       "      <td>1141</td>\n",
       "      <td>03/31/1979</td>\n",
       "      <td>1978</td>\n",
       "      <td>73</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.092000</td>\n",
       "      <td>2.217033</td>\n",
       "      <td>0.293956</td>\n",
       "      <td>-25.645299</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.274608</td>\n",
       "      <td>1.624625</td>\n",
       "      <td>0.041796</td>\n",
       "      <td>-0.013952</td>\n",
       "      <td>0.100517</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>11260</td>\n",
       "      <td>3022</td>\n",
       "      <td>12/31/1978</td>\n",
       "      <td>1978</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>548.843068</td>\n",
       "      <td>3.730699</td>\n",
       "      <td>-0.372784</td>\n",
       "      <td>-10.001630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753830</td>\n",
       "      <td>2.611047</td>\n",
       "      <td>1.942549e-08</td>\n",
       "      <td>3.251945e-08</td>\n",
       "      <td>0.972871</td>\n",
       "      <td>0.946478</td>\n",
       "      <td>0.100623</td>\n",
       "      <td>0.186219</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>11308</td>\n",
       "      <td>3144</td>\n",
       "      <td>12/31/1978</td>\n",
       "      <td>1978</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5419.612673</td>\n",
       "      <td>0.320984</td>\n",
       "      <td>0.069136</td>\n",
       "      <td>8.871277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579809</td>\n",
       "      <td>0.418106</td>\n",
       "      <td>3.221865e-09</td>\n",
       "      <td>1.362653e-07</td>\n",
       "      <td>0.744968</td>\n",
       "      <td>0.554978</td>\n",
       "      <td>0.261126</td>\n",
       "      <td>0.236456</td>\n",
       "      <td>0.020792</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341710</th>\n",
       "      <td>91556</td>\n",
       "      <td>9248</td>\n",
       "      <td>01/31/1986</td>\n",
       "      <td>1985</td>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>334.672000</td>\n",
       "      <td>0.377522</td>\n",
       "      <td>0.021080</td>\n",
       "      <td>43.089022</td>\n",
       "      <td>...</td>\n",
       "      <td>1.038543</td>\n",
       "      <td>21.132059</td>\n",
       "      <td>4.597927e-08</td>\n",
       "      <td>8.618303e-09</td>\n",
       "      <td>1.151457</td>\n",
       "      <td>1.325853</td>\n",
       "      <td>0.039875</td>\n",
       "      <td>-0.401413</td>\n",
       "      <td>0.068092</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341746</th>\n",
       "      <td>92401</td>\n",
       "      <td>10392</td>\n",
       "      <td>12/31/1985</td>\n",
       "      <td>1985</td>\n",
       "      <td>62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.498999</td>\n",
       "      <td>0.186761</td>\n",
       "      <td>-0.144935</td>\n",
       "      <td>3.140651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448634</td>\n",
       "      <td>1.470119</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.148666e-08</td>\n",
       "      <td>0.785494</td>\n",
       "      <td>0.617001</td>\n",
       "      <td>0.008787</td>\n",
       "      <td>1.368951</td>\n",
       "      <td>0.071624</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341756</th>\n",
       "      <td>92575</td>\n",
       "      <td>10728</td>\n",
       "      <td>12/31/1985</td>\n",
       "      <td>1985</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.313250</td>\n",
       "      <td>0.535880</td>\n",
       "      <td>0.050494</td>\n",
       "      <td>6.586239</td>\n",
       "      <td>...</td>\n",
       "      <td>2.181790</td>\n",
       "      <td>0.154151</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.657895e+01</td>\n",
       "      <td>-0.242451</td>\n",
       "      <td>0.058782</td>\n",
       "      <td>-0.005332</td>\n",
       "      <td>0.886406</td>\n",
       "      <td>0.034116</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341758</th>\n",
       "      <td>92591</td>\n",
       "      <td>10754</td>\n",
       "      <td>12/31/1985</td>\n",
       "      <td>1985</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.069216</td>\n",
       "      <td>0.099012</td>\n",
       "      <td>0.013671</td>\n",
       "      <td>11.071744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637323</td>\n",
       "      <td>1.596371</td>\n",
       "      <td>4.564405e-07</td>\n",
       "      <td>4.458273e-08</td>\n",
       "      <td>1.538611</td>\n",
       "      <td>2.367324</td>\n",
       "      <td>0.035727</td>\n",
       "      <td>0.251801</td>\n",
       "      <td>0.087214</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341782</th>\n",
       "      <td>93316</td>\n",
       "      <td>11768</td>\n",
       "      <td>09/30/1985</td>\n",
       "      <td>1985</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.152500</td>\n",
       "      <td>0.728042</td>\n",
       "      <td>0.101846</td>\n",
       "      <td>-18.323574</td>\n",
       "      <td>...</td>\n",
       "      <td>1.897624</td>\n",
       "      <td>6.189418</td>\n",
       "      <td>1.996180e-05</td>\n",
       "      <td>2.210526e+00</td>\n",
       "      <td>0.311242</td>\n",
       "      <td>0.096872</td>\n",
       "      <td>-0.003214</td>\n",
       "      <td>0.858466</td>\n",
       "      <td>0.042988</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28988 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        permno  gvkey   adatadate  fyear  sic2     spi        mve_f        bm  \\\n",
       "31       10698   2040  12/31/1978   1978    12  0.0009    68.839375  1.269332   \n",
       "41       10866   2436  10/31/1978   1978    56 -0.0038   180.971833  1.357946   \n",
       "46       10912   1141  03/31/1979   1978    73  0.0000     1.092000  2.217033   \n",
       "60       11260   3022  12/31/1978   1978    37  0.0000   548.843068  3.730699   \n",
       "62       11308   3144  12/31/1978   1978    20  0.0000  5419.612673  0.320984   \n",
       "...        ...    ...         ...    ...   ...     ...          ...       ...   \n",
       "341710   91556   9248  01/31/1986   1985    56     NaN   334.672000  0.377522   \n",
       "341746   92401  10392  12/31/1985   1985    62     NaN     5.498999  0.186761   \n",
       "341756   92575  10728  12/31/1985   1985    27     NaN    49.313250  0.535880   \n",
       "341758   92591  10754  12/31/1985   1985    35     NaN    12.069216  0.099012   \n",
       "341782   93316  11768  09/30/1985   1985    36     NaN    23.152500  0.728042   \n",
       "\n",
       "              ep     cashpr  ...  std_dolvol   std_turn           ill  \\\n",
       "31      0.116634 -13.067487  ...    0.753815   2.894962  4.539289e-08   \n",
       "41      0.184896  -6.925387  ...    0.759439   0.317240  1.205612e-07   \n",
       "46      0.293956 -25.645299  ...         NaN        NaN           NaN   \n",
       "60     -0.372784 -10.001630  ...    0.753830   2.611047  1.942549e-08   \n",
       "62      0.069136   8.871277  ...    0.579809   0.418106  3.221865e-09   \n",
       "...          ...        ...  ...         ...        ...           ...   \n",
       "341710  0.021080  43.089022  ...    1.038543  21.132059  4.597927e-08   \n",
       "341746 -0.144935   3.140651  ...    0.448634   1.470119  0.000000e+00   \n",
       "341756  0.050494   6.586239  ...    2.181790   0.154151  0.000000e+00   \n",
       "341758  0.013671  11.071744  ...    0.637323   1.596371  4.564405e-07   \n",
       "341782  0.101846 -18.323574  ...    1.897624   6.189418  1.996180e-05   \n",
       "\n",
       "           zerotrade      beta    betasq      rsq1  pricedelay   idiovol  \\\n",
       "31      2.609996e-08  0.589829  0.347898  0.063211    0.037030  0.038551   \n",
       "41      2.628598e-07  0.609418  0.371390  0.122947   -0.228825  0.028520   \n",
       "46               NaN  1.274608  1.624625  0.041796   -0.013952  0.100517   \n",
       "60      3.251945e-08  0.972871  0.946478  0.100623    0.186219  0.049505   \n",
       "62      1.362653e-07  0.744968  0.554978  0.261126    0.236456  0.020792   \n",
       "...              ...       ...       ...       ...         ...       ...   \n",
       "341710  8.618303e-09  1.151457  1.325853  0.039875   -0.401413  0.068092   \n",
       "341746  3.148666e-08  0.785494  0.617001  0.008787    1.368951  0.071624   \n",
       "341756  1.657895e+01 -0.242451  0.058782 -0.005332    0.886406  0.034116   \n",
       "341758  4.458273e-08  1.538611  2.367324  0.035727    0.251801  0.087214   \n",
       "341782  2.210526e+00  0.311242  0.096872 -0.003214    0.858466  0.042988   \n",
       "\n",
       "        pyear  \n",
       "31       1980  \n",
       "41       1980  \n",
       "46       1980  \n",
       "60       1980  \n",
       "62       1980  \n",
       "...       ...  \n",
       "341710   1986  \n",
       "341746   1986  \n",
       "341756   1986  \n",
       "341758   1986  \n",
       "341782   1986  \n",
       "\n",
       "[28988 rows x 158 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42f97786-eeef-461a-886e-55a30e621f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>adatadate</th>\n",
       "      <th>fyear</th>\n",
       "      <th>sic2</th>\n",
       "      <th>spi</th>\n",
       "      <th>mve_f</th>\n",
       "      <th>bm</th>\n",
       "      <th>ep</th>\n",
       "      <th>cashpr</th>\n",
       "      <th>...</th>\n",
       "      <th>std_dolvol</th>\n",
       "      <th>std_turn</th>\n",
       "      <th>ill</th>\n",
       "      <th>zerotrade</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>rsq1</th>\n",
       "      <th>pricedelay</th>\n",
       "      <th>idiovol</th>\n",
       "      <th>pyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>341786</th>\n",
       "      <td>10027</td>\n",
       "      <td>12305</td>\n",
       "      <td>03/31/1986</td>\n",
       "      <td>1985</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>0.271882</td>\n",
       "      <td>0.041294</td>\n",
       "      <td>4.492978</td>\n",
       "      <td>...</td>\n",
       "      <td>1.564011</td>\n",
       "      <td>8.993560</td>\n",
       "      <td>6.312183e-06</td>\n",
       "      <td>9.545455e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341791</th>\n",
       "      <td>10048</td>\n",
       "      <td>12102</td>\n",
       "      <td>05/31/1986</td>\n",
       "      <td>1985</td>\n",
       "      <td>48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>272.249528</td>\n",
       "      <td>0.131820</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>18.170654</td>\n",
       "      <td>...</td>\n",
       "      <td>1.286789</td>\n",
       "      <td>2.281508</td>\n",
       "      <td>1.393634e-06</td>\n",
       "      <td>4.541301e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341795</th>\n",
       "      <td>10070</td>\n",
       "      <td>12520</td>\n",
       "      <td>03/31/1986</td>\n",
       "      <td>1985</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.527500</td>\n",
       "      <td>0.099704</td>\n",
       "      <td>-0.089442</td>\n",
       "      <td>6.329292</td>\n",
       "      <td>...</td>\n",
       "      <td>1.060865</td>\n",
       "      <td>5.913670</td>\n",
       "      <td>6.913003e-08</td>\n",
       "      <td>1.532746e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341841</th>\n",
       "      <td>10866</td>\n",
       "      <td>2436</td>\n",
       "      <td>10/31/1985</td>\n",
       "      <td>1985</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>573.420000</td>\n",
       "      <td>0.657363</td>\n",
       "      <td>0.085053</td>\n",
       "      <td>-0.080908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681750</td>\n",
       "      <td>1.213941</td>\n",
       "      <td>5.256297e-09</td>\n",
       "      <td>4.533236e-08</td>\n",
       "      <td>0.632797</td>\n",
       "      <td>0.400433</td>\n",
       "      <td>0.065110</td>\n",
       "      <td>0.168126</td>\n",
       "      <td>0.032726</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341846</th>\n",
       "      <td>10912</td>\n",
       "      <td>1141</td>\n",
       "      <td>12/31/1985</td>\n",
       "      <td>1985</td>\n",
       "      <td>73</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.711750</td>\n",
       "      <td>0.534337</td>\n",
       "      <td>0.092091</td>\n",
       "      <td>-1.132083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787355</td>\n",
       "      <td>1.160510</td>\n",
       "      <td>1.166170e-05</td>\n",
       "      <td>1.909091e+00</td>\n",
       "      <td>0.896344</td>\n",
       "      <td>0.803433</td>\n",
       "      <td>0.019788</td>\n",
       "      <td>0.309812</td>\n",
       "      <td>0.078399</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395600</th>\n",
       "      <td>91556</td>\n",
       "      <td>9248</td>\n",
       "      <td>01/31/1987</td>\n",
       "      <td>1986</td>\n",
       "      <td>56</td>\n",
       "      <td>-0.1543</td>\n",
       "      <td>135.124500</td>\n",
       "      <td>0.628642</td>\n",
       "      <td>-0.306747</td>\n",
       "      <td>-13.608253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776736</td>\n",
       "      <td>1.884524</td>\n",
       "      <td>9.858786e-08</td>\n",
       "      <td>3.520751e-08</td>\n",
       "      <td>1.734325</td>\n",
       "      <td>3.007883</td>\n",
       "      <td>0.187160</td>\n",
       "      <td>0.008384</td>\n",
       "      <td>0.079553</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395643</th>\n",
       "      <td>92401</td>\n",
       "      <td>10392</td>\n",
       "      <td>12/31/1986</td>\n",
       "      <td>1986</td>\n",
       "      <td>62</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.203999</td>\n",
       "      <td>0.146518</td>\n",
       "      <td>-0.084623</td>\n",
       "      <td>11.499997</td>\n",
       "      <td>...</td>\n",
       "      <td>1.064012</td>\n",
       "      <td>1.973528</td>\n",
       "      <td>1.668580e-05</td>\n",
       "      <td>5.284194e-08</td>\n",
       "      <td>0.501638</td>\n",
       "      <td>0.251641</td>\n",
       "      <td>0.011662</td>\n",
       "      <td>0.569690</td>\n",
       "      <td>0.077391</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395652</th>\n",
       "      <td>92575</td>\n",
       "      <td>10728</td>\n",
       "      <td>12/31/1986</td>\n",
       "      <td>1986</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.0301</td>\n",
       "      <td>39.026625</td>\n",
       "      <td>0.581321</td>\n",
       "      <td>0.024470</td>\n",
       "      <td>0.544062</td>\n",
       "      <td>...</td>\n",
       "      <td>2.221737</td>\n",
       "      <td>0.404125</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.785000e+01</td>\n",
       "      <td>0.180611</td>\n",
       "      <td>0.032620</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>1.039796</td>\n",
       "      <td>0.040927</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395654</th>\n",
       "      <td>92591</td>\n",
       "      <td>10754</td>\n",
       "      <td>12/31/1986</td>\n",
       "      <td>1986</td>\n",
       "      <td>35</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>26.523000</td>\n",
       "      <td>0.056215</td>\n",
       "      <td>0.009727</td>\n",
       "      <td>45.285132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728634</td>\n",
       "      <td>0.847776</td>\n",
       "      <td>6.489236e-07</td>\n",
       "      <td>8.257556e-08</td>\n",
       "      <td>1.456934</td>\n",
       "      <td>2.122657</td>\n",
       "      <td>0.103737</td>\n",
       "      <td>0.139770</td>\n",
       "      <td>0.089248</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395681</th>\n",
       "      <td>93316</td>\n",
       "      <td>11768</td>\n",
       "      <td>09/30/1986</td>\n",
       "      <td>1986</td>\n",
       "      <td>36</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>17.941500</td>\n",
       "      <td>0.968091</td>\n",
       "      <td>0.028259</td>\n",
       "      <td>-2.883953</td>\n",
       "      <td>...</td>\n",
       "      <td>1.739907</td>\n",
       "      <td>6.753378</td>\n",
       "      <td>6.310750e-05</td>\n",
       "      <td>4.200000e+00</td>\n",
       "      <td>1.307264</td>\n",
       "      <td>1.708938</td>\n",
       "      <td>0.108312</td>\n",
       "      <td>0.099693</td>\n",
       "      <td>0.075732</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4881 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        permno  gvkey   adatadate  fyear  sic2     spi       mve_f        bm  \\\n",
       "341786   10027  12305  03/31/1986   1985    73     NaN   25.500000  0.271882   \n",
       "341791   10048  12102  05/31/1986   1985    48     NaN  272.249528  0.131820   \n",
       "341795   10070  12520  03/31/1986   1985    35     NaN   29.527500  0.099704   \n",
       "341841   10866   2436  10/31/1985   1985    56  0.0000  573.420000  0.657363   \n",
       "341846   10912   1141  12/31/1985   1985    73  0.0000    5.711750  0.534337   \n",
       "...        ...    ...         ...    ...   ...     ...         ...       ...   \n",
       "395600   91556   9248  01/31/1987   1986    56 -0.1543  135.124500  0.628642   \n",
       "395643   92401  10392  12/31/1986   1986    62  0.0000    6.203999  0.146518   \n",
       "395652   92575  10728  12/31/1986   1986    27 -0.0301   39.026625  0.581321   \n",
       "395654   92591  10754  12/31/1986   1986    35  0.0000   26.523000  0.056215   \n",
       "395681   93316  11768  09/30/1986   1986    36  0.0000   17.941500  0.968091   \n",
       "\n",
       "              ep     cashpr  ...  std_dolvol  std_turn           ill  \\\n",
       "341786  0.041294   4.492978  ...    1.564011  8.993560  6.312183e-06   \n",
       "341791  0.003023  18.170654  ...    1.286789  2.281508  1.393634e-06   \n",
       "341795 -0.089442   6.329292  ...    1.060865  5.913670  6.913003e-08   \n",
       "341841  0.085053  -0.080908  ...    0.681750  1.213941  5.256297e-09   \n",
       "341846  0.092091  -1.132083  ...    0.787355  1.160510  1.166170e-05   \n",
       "...          ...        ...  ...         ...       ...           ...   \n",
       "395600 -0.306747 -13.608253  ...    0.776736  1.884524  9.858786e-08   \n",
       "395643 -0.084623  11.499997  ...    1.064012  1.973528  1.668580e-05   \n",
       "395652  0.024470   0.544062  ...    2.221737  0.404125  0.000000e+00   \n",
       "395654  0.009727  45.285132  ...    0.728634  0.847776  6.489236e-07   \n",
       "395681  0.028259  -2.883953  ...    1.739907  6.753378  6.310750e-05   \n",
       "\n",
       "           zerotrade      beta    betasq      rsq1  pricedelay   idiovol  \\\n",
       "341786  9.545455e-01       NaN       NaN       NaN         NaN       NaN   \n",
       "341791  4.541301e-08       NaN       NaN       NaN         NaN       NaN   \n",
       "341795  1.532746e-08       NaN       NaN       NaN         NaN       NaN   \n",
       "341841  4.533236e-08  0.632797  0.400433  0.065110    0.168126  0.032726   \n",
       "341846  1.909091e+00  0.896344  0.803433  0.019788    0.309812  0.078399   \n",
       "...              ...       ...       ...       ...         ...       ...   \n",
       "395600  3.520751e-08  1.734325  3.007883  0.187160    0.008384  0.079553   \n",
       "395643  5.284194e-08  0.501638  0.251641  0.011662    0.569690  0.077391   \n",
       "395652  1.785000e+01  0.180611  0.032620  0.001167    1.039796  0.040927   \n",
       "395654  8.257556e-08  1.456934  2.122657  0.103737    0.139770  0.089248   \n",
       "395681  4.200000e+00  1.307264  1.708938  0.108312    0.099693  0.075732   \n",
       "\n",
       "        pyear  \n",
       "341786   1987  \n",
       "341791   1987  \n",
       "341795   1987  \n",
       "341841   1987  \n",
       "341846   1987  \n",
       "...       ...  \n",
       "395600   1987  \n",
       "395643   1987  \n",
       "395652   1987  \n",
       "395654   1987  \n",
       "395681   1987  \n",
       "\n",
       "[4881 rows x 158 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "844bd255-61fd-47ba-b400-59462d0ec0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train_data, test_data, features, target, pipeline):\n",
    "    # Train data\n",
    "    x_train = train_data.loc[:, features]\n",
    "    y_train = train_data.loc[:, target]\n",
    "    \n",
    "    # Fit the pipeline to the train data\n",
    "    pipeline.fit(x_train)\n",
    "    x_train_tf = pipeline.transform(x_train)\n",
    "    x_train_tf = x_train_tf[:, :-2]\n",
    "    \n",
    "    # Test data\n",
    "    x_test = test_data.loc[:, features]\n",
    "    y_test = test_data.loc[:, target]\n",
    "    \n",
    "    # Fit the pipeline to the test data\n",
    "    x_test_tf = pipeline.transform(x_test)\n",
    "    x_test_tf = x_test_tf[:, :-2]\n",
    "    \n",
    "    # Transform data into numpy array as type float32\n",
    "    x_train_tf = x_train_tf.astype(np.float32)\n",
    "    y_train_tf = y_train.to_numpy(np.float32)\n",
    "    x_test_tf = x_test_tf.astype(np.float32)\n",
    "    y_test_tf = y_test.to_numpy(np.float32)\n",
    "    \n",
    "    # # Transform them to tensor floats\n",
    "    x_train_tf = torch.tensor(x_train_tf).float()\n",
    "    y_train_tf = torch.tensor(y_train_tf).float()\n",
    "    x_test_tf = torch.tensor(x_test_tf).float()\n",
    "    y_test_tf = torch.tensor(y_test_tf).float()\n",
    "\n",
    "    print(f'x_train shape: {x_train_tf.shape}')\n",
    "    print(f'y_train shape: {y_train_tf.shape}\\n')\n",
    "    print(f'x_test shape: {x_test_tf.shape}')\n",
    "    print(f'y_test shape: {y_test_tf.shape}\\n')\n",
    "    \n",
    "    return x_train_tf, y_train_tf, x_test_tf, y_test_tf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca0097f4-9664-4654-91fd-0c260a9b118f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zfs/projects/darc/wolee_edehaan_suzienoh-exploratory-ml/kevin/venv/lib/python3.10/site-packages/sklearn/utils/extmath.py:1140: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/zfs/projects/darc/wolee_edehaan_suzienoh-exploratory-ml/kevin/venv/lib/python3.10/site-packages/sklearn/utils/extmath.py:1145: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/zfs/projects/darc/wolee_edehaan_suzienoh-exploratory-ml/kevin/venv/lib/python3.10/site-packages/sklearn/utils/extmath.py:1165: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([28988, 103])\n",
      "y_train shape: torch.Size([28988])\n",
      "\n",
      "x_test shape: torch.Size([4881, 103])\n",
      "y_test shape: torch.Size([4881])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = transform_data(train_data, test_data, deps, target, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ba1ac64-a93a-4704-9881-53325b0d6b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_len = len(con_list) + len(dum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ab72e20-a969-4364-9c93-cdbfed3e0023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuous_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c847ec24-082c-4396-8ad2-9dbcd44c2659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.7609e-01,  7.9748e-01, -4.0289e-01,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  1.0698e+04],\n",
       "        [ 8.1625e-02,  3.1391e-01, -6.1745e-01,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  1.0866e+04],\n",
       "        [ 2.0111e-01,  4.1110e-01,  6.4145e-04,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  1.0912e+04],\n",
       "        ...,\n",
       "        [ 9.1361e-03, -2.4392e-02, -1.0743e+00,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  9.2575e+04],\n",
       "        [ 9.1361e-03, -2.4392e-02, -8.7273e-01,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  9.2591e+04],\n",
       "        [ 9.1361e-03, -2.4392e-02,  7.2840e-01,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  9.3316e+04]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3c4372a-efc4-47de-8791-5eabd267f724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28988, 102])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:, :continuous_len].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be53bdbe-6a2a-4353-8841-9afad54a7c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28988, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:, continuous_len:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa18c5c6-c9b4-4638-8bf6-a6497762556b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.165515e+06\n",
       "mean     5.777522e+04\n",
       "std      2.776163e+04\n",
       "min      1.000100e+04\n",
       "25%      3.141400e+04\n",
       "50%      6.552500e+04\n",
       "75%      8.172100e+04\n",
       "max      9.343600e+04\n",
       "Name: permno, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since permno is already integers, no need to convert str to int for embedding step\n",
    "df1['permno'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6458be-22f8-4502-90c5-004cc9f20dfd",
   "metadata": {},
   "source": [
    "## Finalize data for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4f6768e-b366-4f70-9f39-5026f6d04ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XandYDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43ab6576-e9c3-4069-8abb-883b4ddf5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XandYDataset(Dataset):\n",
    "    def __init__(self, X_continuous_vars, X_embedding_vars, y):\n",
    "        self.X_continuous_vars = X_continuous_vars\n",
    "        if isinstance(X_embedding_vars, torch.Tensor):\n",
    "            self.X_embedding_vars = X_embedding_vars.numpy()\n",
    "        else:\n",
    "            self.X_embedding_vars = X_embedding_vars\n",
    "        self.y = y\n",
    "\n",
    "        # Ensure the categorical variables are strings\n",
    "        self.X_embedding_vars = self.X_embedding_vars.astype(str)\n",
    "        \n",
    "        # Create a mapping dictionary for each unique categorical variable to integer\n",
    "        self.embedding_var_mappings = self._create_mappings(self.X_embedding_vars)\n",
    "\n",
    "    def _create_mappings(self, X_embedding_vars):\n",
    "        mappings = {}\n",
    "        for i in range(X_embedding_vars.shape[1]):\n",
    "            unique_values = set(X_embedding_vars[:, i])\n",
    "            mappings[i] = {val: idx for idx, val in enumerate(unique_values)}\n",
    "        return mappings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_continuous_vars)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_continuous = self.X_continuous_vars[idx]\n",
    "        X_embedding = self.X_embedding_vars[idx]\n",
    "\n",
    "        # Convert each categorical variable to its corresponding integer index\n",
    "        X_embedding_int = torch.tensor([self.embedding_var_mappings[i][val] for i, val in enumerate(X_embedding)], dtype=torch.long)\n",
    "\n",
    "        return X_continuous, X_embedding_int, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "264cd303-645b-47fd-acbc-136da4cdf5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50dc8364-4a0a-431b-85ca-6219276b9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = XandYDataset(x_train[:, :continuous_len], x_train[:, continuous_len:], y_train)\n",
    "test_dataset = XandYDataset(x_test[:, :continuous_len], x_test[:, continuous_len:], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b226ac45-d7d8-4f7f-8950-48ae9f926fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28988, 102])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:, :continuous_len].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1faaad7a-b917-4f70-9213-8e60c334e20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28988, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:, continuous_len:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9552b6b-3f35-4733-904b-457d29bd419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ecc4983-a410-497e-830b-c4e3e40d9869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.8267,  0.9200, -0.9892,  ...,  0.0000,  1.0000,  0.0000],\n",
       "         [-0.8199, -0.4194, -0.9157,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0121, -1.0184, -1.0640,  ...,  0.0000,  1.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.4125,  0.5831,  4.2784,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0065,  0.0153, -0.0284,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.9092, -0.4921, -0.8940,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor([[280],\n",
       "         [405],\n",
       "         [355],\n",
       "         [181],\n",
       "         [231],\n",
       "         [399],\n",
       "         [446],\n",
       "         [331],\n",
       "         [325],\n",
       "         [387],\n",
       "         [422],\n",
       "         [134],\n",
       "         [442],\n",
       "         [ 39],\n",
       "         [ 38],\n",
       "         [ 10],\n",
       "         [ 78],\n",
       "         [ 18],\n",
       "         [169],\n",
       "         [ 24],\n",
       "         [260],\n",
       "         [181],\n",
       "         [193],\n",
       "         [324],\n",
       "         [259],\n",
       "         [355],\n",
       "         [ 49],\n",
       "         [409],\n",
       "         [199],\n",
       "         [332],\n",
       "         [244],\n",
       "         [409]]),\n",
       " tensor([-0.0605,  0.0249,  0.1867,  0.3302, -0.0686,  0.1718, -0.0042, -0.0526,\n",
       "          0.0769, -0.0303,  0.0526, -0.2105,  0.0250,  0.0099,  0.4426,  0.0319,\n",
       "         -0.0044, -0.2051,  0.0769, -0.0410,  0.0000,  0.0465,  0.0057,  0.0769,\n",
       "          0.0578, -0.1173, -0.0127,  0.0643, -0.0400, -0.0395, -0.0882,  0.1680])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "645a55f8-f385-415f-90e1-b67438fde9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_continuous, X_embed, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88d67ff9-ca81-4e17-aad0-5c86dc64aa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 102]) torch.Size([32, 1]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(X_continuous.shape, X_embed.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07a4eced-0a4b-44b4-8f57-7d7377da1c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: 28988\n",
      "test_dataset: 4881\n"
     ]
    }
   ],
   "source": [
    "print(f'train_dataset: {len(train_dataset)}')\n",
    "print(f'test_dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "918cfbaf-6e00-4832-8a58-6971d3760666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.5848, -0.2282, -0.8616, -0.8411, -0.2423, -0.4317, -0.2812, -0.4452,\n",
       "         -0.0032,  0.2094, -0.6951, -0.0164,  0.0296,  0.0331, -0.0057,  0.0416,\n",
       "         -0.4384, -0.3467,  0.0000, -0.3798, -0.1366,  0.0000,  0.0797,  2.6947,\n",
       "         -0.0051, -0.6563, -0.5054,  0.0000,  1.4770,  0.3292, -0.1324, -0.0749,\n",
       "          0.3279,  0.0000,  0.0792, -0.1692,  0.5088, -0.2947, -0.2402, -0.9355,\n",
       "         -0.3051,  2.7384,  0.0134, -0.1643, -0.2729,  0.0301,  1.1708,  0.0123,\n",
       "         -0.5665,  0.7236,  0.4563,  1.9516,  1.6305,  0.0000, -0.8442,  0.3035,\n",
       "         -0.5233, -0.1221, -0.0868, -0.1908,  0.0592, -0.0233,  0.2755,  0.2361,\n",
       "         -0.4321,  0.0602, -0.1774,  0.2683,  0.5245, -0.5166, -0.9901, -0.2778,\n",
       "         -0.0185, -0.4600,  0.4201, -0.8453,  0.5514,  0.5773,  0.7123, -0.2923,\n",
       "         -0.1762, -0.0988, -0.0078,  0.0000, -0.2521, -0.1124, -0.9889, -0.6014,\n",
       "         -0.4406, -0.4670,  0.3297, -0.2738,  2.2471, -0.2442, -0.4818,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]),\n",
       " tensor([255]),\n",
       " tensor(0.2600))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431fade6-f97d-4718-b05a-99d132716d31",
   "metadata": {},
   "source": [
    "## Modeling data with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b367c47b-663d-4ee3-8a2f-072477c775e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_rate=0.5):\n",
    "        super(FlexibleNeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Apply Xavier initialization to the layers\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = self.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)  # This applies dropout\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "775d4b16-f727-4daa-b099-afbcbb66b1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_embeddings, embedding_dim, dropout_rate=0.5):\n",
    "        super(FlexibleNeuralNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer (adjust input_dim to account for embedding_dim)\n",
    "        self.layers.append(nn.Linear(input_dim + embedding_dim, hidden_dim))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        # Activation function, note that nn.ReLU() is not appropriate because of outputing non-negative number only\n",
    "        self.activation = nn.LeakyReLU(negative_slope=4) # nn.Tanh()\n",
    "        \n",
    "        # Apply Xavier initialization to the layers\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x_continuous, x_categorical):\n",
    "        embedded = self.embedding(x_categorical)\n",
    "        embedded = embedded.view(embedded.size(0), -1)  # Flatten the embedding\n",
    "        x = torch.cat((x_continuous, embedded), dim=1)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = self.activation(layer(x))\n",
    "            else:\n",
    "                x = layer(x)  # This applies dropout\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a9bd206d-5317-43b4-8c92-e9dc8ef9f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_function, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for x_continuous, x_embedding_vars, targets in train_loader:\n",
    "            x_continuous, x_embedding_vars, targets = \\\n",
    "                x_continuous.to(device), x_embedding_vars.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # outputs is squeezed from shape [batch_size, 1] to [batch_size]\n",
    "            outputs = model(x_continuous, x_embedding_vars).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d8433d0-5753-4800-aad0-30bd17774a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, loss_function):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_continuous, x_embedding_vars, targets in test_loader:\n",
    "            x_continuous, x_embedding_vars, targets = \\\n",
    "                x_continuous.to(device), x_embedding_vars.to(device), targets.to(device)\n",
    "            # outputs is squeezed from shape [batch_size, 1] to [batch_size]\n",
    "            outputs = model(x_continuous, x_embedding_vars).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    print(f'Average Loss: {average_loss}')\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48f843ed-f8f7-48a9-a5d7-646dfaaa8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bb62e3b0-1659-4d1a-a6ce-ee9be456ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # If the loader provides three values, unpack and ignore the third (targets)\n",
    "            if len(batch) == 3:\n",
    "                x_continuous, x_embedding_vars, _ = batch\n",
    "            else:\n",
    "                x_continuous, x_embedding_vars = batch\n",
    "\n",
    "            x_continuous, x_embedding_vars = x_continuous.to(device), x_embedding_vars.to(device)\n",
    "            # outputs is squeezed from shape [batch_size, 1] to [batch_size]\n",
    "            outputs = model(x_continuous, x_embedding_vars).squeeze()\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0406fcbe-8fd8-4144-abd4-9bbf78cceb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "input_dim = continuous_len\n",
    "hidden_dim = 50\n",
    "output_dim = 1 # 1 for regression\n",
    "num_layers = 2  # Number of linear layers\n",
    "num_embeddings = sample['permno'].nunique()\n",
    "embedding_dim = 5\n",
    "dropout_rate = 0.3\n",
    "lr = 5e-4\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd3ccef0-aa3d-42e9-abb4-d54f457314a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zfs/projects/darc/wolee_edehaan_suzienoh-exploratory-ml/kevin/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = FlexibleNeuralNetwork(input_dim, hidden_dim, output_dim, num_layers, num_embeddings, embedding_dim, dropout_rate)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.L1Loss() # nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # weight_decay for L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e3b4645-d2fe-4ad7-b42e-fcbb0b2c928f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlexibleNeuralNetwork(\n",
       "  (embedding): Embedding(953, 5)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=107, out_features=50, bias=True)\n",
       "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       "  (activation): LeakyReLU(negative_slope=4)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43c3c46a-c0ed-49df-9b27-62a058832730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 5.525010971570383\n",
      "Epoch 2/10, Loss: 1.1195433008552387\n",
      "Epoch 3/10, Loss: 0.15644511839921768\n",
      "Epoch 4/10, Loss: 0.10267533843814669\n",
      "Epoch 5/10, Loss: 0.10199528280440019\n",
      "Epoch 6/10, Loss: 0.10135087264376892\n",
      "Epoch 7/10, Loss: 0.10103924645246654\n",
      "Epoch 8/10, Loss: 0.09985176408067181\n",
      "Epoch 9/10, Loss: 0.09953012656642507\n",
      "Epoch 10/10, Loss: 0.09929188685882302\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "train(model, train_loader, loss_function, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33c5366a-7eee-4bed-9d76-96b8b08dcab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.11947521861959128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11947521861959128"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_loader, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7cdab3f3-4208-4e69-bc85-891799cd48a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_final = train_data.copy()\n",
    "test_data_final = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2e12470-150f-4d8b-a9a1-dfcfee238311",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_final['pred'] = predict(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9e5d91b-5d18-45e2-8f19-1f3d77b9af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_final['pred'] = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d002e1e-2fa8-4613-acf9-3d4af6632f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09666238794759509"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(train_data_final[target], train_data_final['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9818e20e-376e-4dd9-8f67-7f7759b5421b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1193948435462886"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(test_data_final[target], test_data_final['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bba27d5e-4b36-4c5f-bee5-a00fb4220ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ret</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28988.000000</td>\n",
       "      <td>28988.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.016174</td>\n",
       "      <td>0.015639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.139005</td>\n",
       "      <td>0.022614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.733333</td>\n",
       "      <td>-0.372393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.058252</td>\n",
       "      <td>0.010208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.018224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.075862</td>\n",
       "      <td>0.025561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.954545</td>\n",
       "      <td>0.256739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ret          pred\n",
       "count  28988.000000  28988.000000\n",
       "mean       0.016174      0.015639\n",
       "std        0.139005      0.022614\n",
       "min       -0.733333     -0.372393\n",
       "25%       -0.058252      0.010208\n",
       "50%        0.001077      0.018224\n",
       "75%        0.075862      0.025561\n",
       "max        3.954545      0.256739"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_final[[target, 'pred']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c8d4ee4-3168-4338-ba3c-1666664ffe11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ret</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4881.000000</td>\n",
       "      <td>4881.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.008822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.168603</td>\n",
       "      <td>0.048172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.622222</td>\n",
       "      <td>-0.434961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.069799</td>\n",
       "      <td>0.008567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.026787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.307692</td>\n",
       "      <td>0.233608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ret         pred\n",
       "count  4881.000000  4881.000000\n",
       "mean      0.004878     0.008822\n",
       "std       0.168603     0.048172\n",
       "min      -0.622222    -0.434961\n",
       "25%      -0.069799     0.008567\n",
       "50%       0.000000     0.018721\n",
       "75%       0.081967     0.026787\n",
       "max       1.307692     0.233608"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_final[[target, 'pred']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a4ba285f-63ae-4213-82cd-a938c0660aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ret</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>341786</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.009466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341791</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.025469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341795</th>\n",
       "      <td>0.430556</td>\n",
       "      <td>0.042102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341841</th>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.024669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341846</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395600</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.004891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395643</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395652</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.012125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395654</th>\n",
       "      <td>-0.192308</td>\n",
       "      <td>0.014399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395681</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4881 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ret      pred\n",
       "341786  0.090909  0.009466\n",
       "341791  0.066667  0.025469\n",
       "341795  0.430556  0.042102\n",
       "341841  0.072464  0.024669\n",
       "341846  0.000000  0.021313\n",
       "...          ...       ...\n",
       "395600  0.031250  0.004891\n",
       "395643  0.000000  0.018647\n",
       "395652  0.083333  0.012125\n",
       "395654 -0.192308  0.014399\n",
       "395681  0.000000  0.000543\n",
       "\n",
       "[4881 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_final[[target, 'pred']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476e91b-0f8d-4a8c-8b80-52147e96fd2b",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed92037f-f388-4715-943e-58d3f79926c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 10:31:56,322\tINFO worker.py:1753 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1606c88af929493d989c9794c710711a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.10.12</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.24.0</b></td>\n",
       "    </tr>\n",
       "    \n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.10.12', ray_version='2.24.0', ray_commit='cfea8b29800afc76823bd1a171657010f4eb96bb')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Ray\n",
    "ray.init(num_cpus=10, num_gpus=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93335f1a-ae6d-4833-bc09-e7e687fb9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, loss_function, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_continuous, x_embedding_vars, targets in test_loader:\n",
    "            x_continuous, x_embedding_vars, targets = x_continuous.to(device), x_embedding_vars.to(device), targets.to(device)\n",
    "            outputs = model(x_continuous, x_embedding_vars).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1cfa6bae-af43-4916-90b6-4d43ad6792fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fnn(config, train_loader, test_loader, ray_tuning=True):\n",
    "    device = torch.device(\"cuda\" if config[\"gpu\"] > 0 else \"cpu\")\n",
    "    input_dim = config[\"input_dim\"]\n",
    "    hidden_dim = config[\"hidden_dim\"]\n",
    "    output_dim = 1\n",
    "    num_layers = config[\"num_layers\"]\n",
    "    num_embeddings = config[\"num_embeddings\"]\n",
    "    embedding_dim = config[\"embedding_dim\"]\n",
    "    dropout_rate = config[\"dropout_rate\"]\n",
    "    lr = config[\"lr\"]\n",
    "    weight_decay = config[\"weight_decay\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "\n",
    "    model = FlexibleNeuralNetwork(input_dim, hidden_dim, output_dim, num_layers, num_embeddings, embedding_dim, dropout_rate)\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_function = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for x_continuous, x_embedding_vars, targets in train_loader:\n",
    "                x_continuous, x_embedding_vars, targets = x_continuous.to(device), x_embedding_vars.to(device), targets.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_continuous, x_embedding_vars).squeeze()\n",
    "                loss = loss_function(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = running_loss / len(train_loader)\n",
    "            avg_test_loss = evaluate(model, test_loader, loss_function, device)\n",
    "            metrics = {\n",
    "                'avg_train_loss': avg_train_loss,\n",
    "                'avg_test_loss': avg_test_loss,\n",
    "            }\n",
    "            if ray_tuning:\n",
    "                ray.train.report(metrics=metrics)\n",
    "            else:\n",
    "                logger.info(f'Epoch {epoch + 1}/{num_epochs}, metrics: {metrics}')\n",
    "\n",
    "    except Exception as e:\n",
    "        metrics = {\n",
    "            'avg_train_loss': float('inf'),\n",
    "            'avg_test_loss': float('inf'),\n",
    "        }\n",
    "        logger.error(f\"Training failed with exception: {e}\")\n",
    "        if ray_tuning:\n",
    "            ray.train.report(metrics=metrics)\n",
    "        else:\n",
    "            logger.error(f\"Training failed with exception: {e}\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e373c5a2-5a9f-448b-9d1c-175fd9490368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_trial(train_loader, test_loader, num_samples=10, max_num_epochs=10, gpus_per_trial=0):\n",
    "    config = {\n",
    "        \"input_dim\": continuous_len,\n",
    "        \"hidden_dim\": tune.choice([i for i in range(10, 201, 10)]),\n",
    "        \"num_layers\": tune.choice([1, 2, 3]),\n",
    "        \"num_embeddings\": sample['permno'].nunique(),\n",
    "        \"embedding_dim\": tune.choice([i for i in range(1, 50, 5)]),\n",
    "        \"dropout_rate\": tune.uniform(0.01, 0.7),\n",
    "        \"lr\": tune.loguniform(1e-6, 1e-2),\n",
    "        \"weight_decay\": tune.loguniform(1e-6, 1e-3),\n",
    "        \"num_epochs\": max_num_epochs,\n",
    "        \"gpu\": gpus_per_trial,\n",
    "    }\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"avg_test_loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    reporter = CLIReporter(\n",
    "        metric_columns=[\"average_train_loss\", \"avg_test_loss\", \"training_iteration\"])\n",
    "    \n",
    "    result = tune.run(\n",
    "        tune.with_parameters(train_fnn, train_loader=train_loader, test_loader=test_loader),\n",
    "        resources_per_trial={\"cpu\": 1, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        storage_path=\"/zfs/projects/darc/wolee_edehaan_suzienoh-exploratory-ml/kevin/ray_results\",\n",
    "        verbose=3, # to turn off a lot of messages\n",
    "    )\n",
    "    \n",
    "    best_trial = result.get_best_trial(\"avg_test_loss\", \"min\", \"last\")\n",
    "    logger.info(f\"Best trial config: {best_trial.config}\")\n",
    "    logger.info(f\"Best trial training loss: {best_trial.last_result['avg_train_loss']}\")\n",
    "    logger.info(f\"Best trial testing loss: {best_trial.last_result['avg_test_loss']}\")\n",
    "    \n",
    "    return best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf1656d9-1879-45bb-a5d9-53de67dfffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-06-20 10:31:57 (running for 00:00:00.33)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Logical resource usage: 0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (40 PENDING)\n",
      "+-----------------------+----------+-------+----------------+-----------------+--------------+-------------+--------------+----------------+\n",
      "| Trial name            | status   | loc   |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |\n",
      "|-----------------------+----------+-------+----------------+-----------------+--------------+-------------+--------------+----------------|\n",
      "| train_fnn_fe339_00000 | PENDING  |       |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |\n",
      "| train_fnn_fe339_00001 | PENDING  |       |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |\n",
      "| train_fnn_fe339_00002 | PENDING  |       |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |\n",
      "| train_fnn_fe339_00003 | PENDING  |       |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |\n",
      "| train_fnn_fe339_00004 | PENDING  |       |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |\n",
      "| train_fnn_fe339_00005 | PENDING  |       |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |\n",
      "| train_fnn_fe339_00006 | PENDING  |       |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |\n",
      "| train_fnn_fe339_00007 | PENDING  |       |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |\n",
      "| train_fnn_fe339_00008 | PENDING  |       |      0.422551  |              16 |          100 | 0.00486964  |            2 |    1.8428e-06  |\n",
      "| train_fnn_fe339_00009 | PENDING  |       |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |\n",
      "| train_fnn_fe339_00010 | PENDING  |       |      0.0207891 |               1 |          130 | 4.93862e-05 |            1 |    1.52983e-05 |\n",
      "| train_fnn_fe339_00011 | PENDING  |       |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |\n",
      "| train_fnn_fe339_00012 | PENDING  |       |      0.440076  |              41 |           90 | 2.10665e-05 |            3 |    1.55123e-06 |\n",
      "| train_fnn_fe339_00013 | PENDING  |       |      0.199558  |              36 |           50 | 0.000175782 |            3 |    1.40858e-05 |\n",
      "| train_fnn_fe339_00014 | PENDING  |       |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |\n",
      "| train_fnn_fe339_00015 | PENDING  |       |      0.313142  |              46 |          150 | 6.41028e-06 |            2 |    0.000486733 |\n",
      "| train_fnn_fe339_00016 | PENDING  |       |      0.182012  |              21 |           40 | 4.38058e-05 |            2 |    0.000184779 |\n",
      "| train_fnn_fe339_00017 | PENDING  |       |      0.121243  |              11 |           70 | 0.00523348  |            3 |    0.000265681 |\n",
      "| train_fnn_fe339_00018 | PENDING  |       |      0.0942407 |              46 |           60 | 2.6619e-05  |            2 |    0.000525394 |\n",
      "| train_fnn_fe339_00019 | PENDING  |       |      0.220299  |              41 |          200 | 4.55642e-06 |            1 |    4.00192e-05 |\n",
      "+-----------------------+----------+-------+----------------+-----------------+--------------+-------------+--------------+----------------+\n",
      "... 20 more trials not shown (20 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:32:02 (running for 00:00:05.36)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (40 PENDING)\n",
      "+-----------------------+----------+-------+----------------+-----------------+--------------+-------------+--------------+----------------+\n",
      "| Trial name            | status   | loc   |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |\n",
      "|-----------------------+----------+-------+----------------+-----------------+--------------+-------------+--------------+----------------|\n",
      "| train_fnn_fe339_00000 | PENDING  |       |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |\n",
      "| train_fnn_fe339_00001 | PENDING  |       |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |\n",
      "| train_fnn_fe339_00002 | PENDING  |       |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |\n",
      "| train_fnn_fe339_00003 | PENDING  |       |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |\n",
      "| train_fnn_fe339_00004 | PENDING  |       |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |\n",
      "| train_fnn_fe339_00005 | PENDING  |       |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |\n",
      "| train_fnn_fe339_00006 | PENDING  |       |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |\n",
      "| train_fnn_fe339_00007 | PENDING  |       |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |\n",
      "| train_fnn_fe339_00008 | PENDING  |       |      0.422551  |              16 |          100 | 0.00486964  |            2 |    1.8428e-06  |\n",
      "| train_fnn_fe339_00009 | PENDING  |       |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |\n",
      "| train_fnn_fe339_00010 | PENDING  |       |      0.0207891 |               1 |          130 | 4.93862e-05 |            1 |    1.52983e-05 |\n",
      "| train_fnn_fe339_00011 | PENDING  |       |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |\n",
      "| train_fnn_fe339_00012 | PENDING  |       |      0.440076  |              41 |           90 | 2.10665e-05 |            3 |    1.55123e-06 |\n",
      "| train_fnn_fe339_00013 | PENDING  |       |      0.199558  |              36 |           50 | 0.000175782 |            3 |    1.40858e-05 |\n",
      "| train_fnn_fe339_00014 | PENDING  |       |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |\n",
      "| train_fnn_fe339_00015 | PENDING  |       |      0.313142  |              46 |          150 | 6.41028e-06 |            2 |    0.000486733 |\n",
      "| train_fnn_fe339_00016 | PENDING  |       |      0.182012  |              21 |           40 | 4.38058e-05 |            2 |    0.000184779 |\n",
      "| train_fnn_fe339_00017 | PENDING  |       |      0.121243  |              11 |           70 | 0.00523348  |            3 |    0.000265681 |\n",
      "| train_fnn_fe339_00018 | PENDING  |       |      0.0942407 |              46 |           60 | 2.6619e-05  |            2 |    0.000525394 |\n",
      "| train_fnn_fe339_00019 | PENDING  |       |      0.220299  |              41 |          200 | 4.55642e-06 |            1 |    4.00192e-05 |\n",
      "+-----------------------+----------+-------+----------------+-----------------+--------------+-------------+--------------+----------------+\n",
      "... 20 more trials not shown (20 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th style=\"text-align: right;\">  avg_test_loss</th><th style=\"text-align: right;\">  avg_train_loss</th><th>checkpoint_dir_name  </th><th>date               </th><th>done  </th><th>hostname  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fnn_fe339_00000</td><td style=\"text-align: right;\">      11.9615  </td><td style=\"text-align: right;\">       6.18317  </td><td>                     </td><td>2024-06-20_10-32-06</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">601613</td><td style=\"text-align: right;\">             3.0964 </td><td style=\"text-align: right;\">           3.0964 </td><td style=\"text-align: right;\">       3.0964 </td><td style=\"text-align: right;\"> 1718904726</td><td style=\"text-align: right;\">                   1</td><td>fe339_00000</td></tr>\n",
       "<tr><td>train_fnn_fe339_00001</td><td style=\"text-align: right;\">       0.470559</td><td style=\"text-align: right;\">       1.03694  </td><td>                     </td><td>2024-06-20_10-32-07</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         2</td><td>171.67.96.198</td><td style=\"text-align: right;\">601614</td><td style=\"text-align: right;\">             4.74519</td><td style=\"text-align: right;\">           1.46096</td><td style=\"text-align: right;\">       4.74519</td><td style=\"text-align: right;\"> 1718904727</td><td style=\"text-align: right;\">                   2</td><td>fe339_00001</td></tr>\n",
       "<tr><td>train_fnn_fe339_00002</td><td style=\"text-align: right;\">      88.2948  </td><td style=\"text-align: right;\">      60.2973   </td><td>                     </td><td>2024-06-20_10-32-06</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">601615</td><td style=\"text-align: right;\">             3.14271</td><td style=\"text-align: right;\">           3.14271</td><td style=\"text-align: right;\">       3.14271</td><td style=\"text-align: right;\"> 1718904726</td><td style=\"text-align: right;\">                   1</td><td>fe339_00002</td></tr>\n",
       "<tr><td>train_fnn_fe339_00003</td><td style=\"text-align: right;\">     171.207   </td><td style=\"text-align: right;\">      69.0931   </td><td>                     </td><td>2024-06-20_10-32-08</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         2</td><td>171.67.96.198</td><td style=\"text-align: right;\">601616</td><td style=\"text-align: right;\">             5.31816</td><td style=\"text-align: right;\">           1.726  </td><td style=\"text-align: right;\">       5.31816</td><td style=\"text-align: right;\"> 1718904728</td><td style=\"text-align: right;\">                   2</td><td>fe339_00003</td></tr>\n",
       "<tr><td>train_fnn_fe339_00004</td><td style=\"text-align: right;\">       0.19882 </td><td style=\"text-align: right;\">       0.137026 </td><td>                     </td><td>2024-06-20_10-32-25</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                        16</td><td>171.67.96.198</td><td style=\"text-align: right;\">601618</td><td style=\"text-align: right;\">            22.2446 </td><td style=\"text-align: right;\">           1.2634 </td><td style=\"text-align: right;\">      22.2446 </td><td style=\"text-align: right;\"> 1718904745</td><td style=\"text-align: right;\">                  16</td><td>fe339_00004</td></tr>\n",
       "<tr><td>train_fnn_fe339_00005</td><td style=\"text-align: right;\">      17.3654  </td><td style=\"text-align: right;\">      20.7246   </td><td>                     </td><td>2024-06-20_10-32-06</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">601617</td><td style=\"text-align: right;\">             3.21011</td><td style=\"text-align: right;\">           3.21011</td><td style=\"text-align: right;\">       3.21011</td><td style=\"text-align: right;\"> 1718904726</td><td style=\"text-align: right;\">                   1</td><td>fe339_00005</td></tr>\n",
       "<tr><td>train_fnn_fe339_00006</td><td style=\"text-align: right;\">      46.3597  </td><td style=\"text-align: right;\">     173.954    </td><td>                     </td><td>2024-06-20_10-32-06</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">601619</td><td style=\"text-align: right;\">             3.31408</td><td style=\"text-align: right;\">           3.31408</td><td style=\"text-align: right;\">       3.31408</td><td style=\"text-align: right;\"> 1718904726</td><td style=\"text-align: right;\">                   1</td><td>fe339_00006</td></tr>\n",
       "<tr><td>train_fnn_fe339_00007</td><td style=\"text-align: right;\">      91.6444  </td><td style=\"text-align: right;\">      68.7719   </td><td>                     </td><td>2024-06-20_10-32-06</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">601620</td><td style=\"text-align: right;\">             3.30781</td><td style=\"text-align: right;\">           3.30781</td><td style=\"text-align: right;\">       3.30781</td><td style=\"text-align: right;\"> 1718904726</td><td style=\"text-align: right;\">                   1</td><td>fe339_00007</td></tr>\n",
       "<tr><td>train_fnn_fe339_00008</td><td style=\"text-align: right;\">       0.577009</td><td style=\"text-align: right;\">       0.144541 </td><td>                     </td><td>2024-06-20_10-32-10</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         4</td><td>171.67.96.198</td><td style=\"text-align: right;\">601621</td><td style=\"text-align: right;\">             7.42797</td><td style=\"text-align: right;\">           1.42345</td><td style=\"text-align: right;\">       7.42797</td><td style=\"text-align: right;\"> 1718904730</td><td style=\"text-align: right;\">                   4</td><td>fe339_00008</td></tr>\n",
       "<tr><td>train_fnn_fe339_00009</td><td style=\"text-align: right;\">       0.119962</td><td style=\"text-align: right;\">       0.0932545</td><td>                     </td><td>2024-06-20_10-32-40</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                        30</td><td>171.67.96.198</td><td style=\"text-align: right;\">601622</td><td style=\"text-align: right;\">            37.2523 </td><td style=\"text-align: right;\">           1.18551</td><td style=\"text-align: right;\">      37.2523 </td><td style=\"text-align: right;\"> 1718904760</td><td style=\"text-align: right;\">                  30</td><td>fe339_00009</td></tr>\n",
       "<tr><td>train_fnn_fe339_00010</td><td style=\"text-align: right;\">       1.90933 </td><td style=\"text-align: right;\">       1.84334  </td><td>                     </td><td>2024-06-20_10-32-14</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">603302</td><td style=\"text-align: right;\">             2.4934 </td><td style=\"text-align: right;\">           2.4934 </td><td style=\"text-align: right;\">       2.4934 </td><td style=\"text-align: right;\"> 1718904734</td><td style=\"text-align: right;\">                   1</td><td>fe339_00010</td></tr>\n",
       "<tr><td>train_fnn_fe339_00011</td><td style=\"text-align: right;\">       0.120435</td><td style=\"text-align: right;\">       0.100982 </td><td>                     </td><td>2024-06-20_10-33-01</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                        30</td><td>171.67.96.198</td><td style=\"text-align: right;\">603303</td><td style=\"text-align: right;\">            49.3901 </td><td style=\"text-align: right;\">           1.59412</td><td style=\"text-align: right;\">      49.3901 </td><td style=\"text-align: right;\"> 1718904781</td><td style=\"text-align: right;\">                  30</td><td>fe339_00011</td></tr>\n",
       "<tr><td>train_fnn_fe339_00012</td><td style=\"text-align: right;\">      43.4836  </td><td style=\"text-align: right;\">      82.0127   </td><td>                     </td><td>2024-06-20_10-32-15</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">603433</td><td style=\"text-align: right;\">             2.8483 </td><td style=\"text-align: right;\">           2.8483 </td><td style=\"text-align: right;\">       2.8483 </td><td style=\"text-align: right;\"> 1718904735</td><td style=\"text-align: right;\">                   1</td><td>fe339_00012</td></tr>\n",
       "<tr><td>train_fnn_fe339_00013</td><td style=\"text-align: right;\">       8.08791 </td><td style=\"text-align: right;\">      22.2742   </td><td>                     </td><td>2024-06-20_10-32-14</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">603436</td><td style=\"text-align: right;\">             2.78063</td><td style=\"text-align: right;\">           2.78063</td><td style=\"text-align: right;\">       2.78063</td><td style=\"text-align: right;\"> 1718904734</td><td style=\"text-align: right;\">                   1</td><td>fe339_00013</td></tr>\n",
       "<tr><td>train_fnn_fe339_00014</td><td style=\"text-align: right;\">       0.133924</td><td style=\"text-align: right;\">       0.0949508</td><td>                     </td><td>2024-06-20_10-32-48</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                        30</td><td>171.67.96.198</td><td style=\"text-align: right;\">603501</td><td style=\"text-align: right;\">            36.3403 </td><td style=\"text-align: right;\">           1.16592</td><td style=\"text-align: right;\">      36.3403 </td><td style=\"text-align: right;\"> 1718904768</td><td style=\"text-align: right;\">                  30</td><td>fe339_00014</td></tr>\n",
       "<tr><td>train_fnn_fe339_00015</td><td style=\"text-align: right;\">      14.2128  </td><td style=\"text-align: right;\">      23.0194   </td><td>                     </td><td>2024-06-20_10-32-16</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">603772</td><td style=\"text-align: right;\">             2.71234</td><td style=\"text-align: right;\">           2.71234</td><td style=\"text-align: right;\">       2.71234</td><td style=\"text-align: right;\"> 1718904736</td><td style=\"text-align: right;\">                   1</td><td>fe339_00015</td></tr>\n",
       "<tr><td>train_fnn_fe339_00016</td><td style=\"text-align: right;\">      10.5261  </td><td style=\"text-align: right;\">      11.6641   </td><td>                     </td><td>2024-06-20_10-32-15</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">603773</td><td style=\"text-align: right;\">             2.56652</td><td style=\"text-align: right;\">           2.56652</td><td style=\"text-align: right;\">       2.56652</td><td style=\"text-align: right;\"> 1718904735</td><td style=\"text-align: right;\">                   1</td><td>fe339_00016</td></tr>\n",
       "<tr><td>train_fnn_fe339_00017</td><td style=\"text-align: right;\">       1.48382 </td><td style=\"text-align: right;\">       1.23689  </td><td>                     </td><td>2024-06-20_10-32-41</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                        16</td><td>171.67.96.198</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\">            25.728  </td><td style=\"text-align: right;\">           1.56108</td><td style=\"text-align: right;\">      25.728  </td><td style=\"text-align: right;\"> 1718904761</td><td style=\"text-align: right;\">                  16</td><td>fe339_00017</td></tr>\n",
       "<tr><td>train_fnn_fe339_00018</td><td style=\"text-align: right;\">       9.85233 </td><td style=\"text-align: right;\">      12.3104   </td><td>                     </td><td>2024-06-20_10-32-22</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">604767</td><td style=\"text-align: right;\">             2.74068</td><td style=\"text-align: right;\">           2.74068</td><td style=\"text-align: right;\">       2.74068</td><td style=\"text-align: right;\"> 1718904742</td><td style=\"text-align: right;\">                   1</td><td>fe339_00018</td></tr>\n",
       "<tr><td>train_fnn_fe339_00019</td><td style=\"text-align: right;\">       5.89094 </td><td style=\"text-align: right;\">       6.20407  </td><td>                     </td><td>2024-06-20_10-32-22</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">604768</td><td style=\"text-align: right;\">             2.64742</td><td style=\"text-align: right;\">           2.64742</td><td style=\"text-align: right;\">       2.64742</td><td style=\"text-align: right;\"> 1718904742</td><td style=\"text-align: right;\">                   1</td><td>fe339_00019</td></tr>\n",
       "<tr><td>train_fnn_fe339_00020</td><td style=\"text-align: right;\">       0.115363</td><td style=\"text-align: right;\">       0.0951021</td><td>                     </td><td>2024-06-20_10-33-07</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                        30</td><td>171.67.96.198</td><td style=\"text-align: right;\">604945</td><td style=\"text-align: right;\">            46.0609 </td><td style=\"text-align: right;\">           1.45871</td><td style=\"text-align: right;\">      46.0609 </td><td style=\"text-align: right;\"> 1718904787</td><td style=\"text-align: right;\">                  30</td><td>fe339_00020</td></tr>\n",
       "<tr><td>train_fnn_fe339_00021</td><td style=\"text-align: right;\">       4.33721 </td><td style=\"text-align: right;\">       2.56641  </td><td>                     </td><td>2024-06-20_10-32-24</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         2</td><td>171.67.96.198</td><td style=\"text-align: right;\">604946</td><td style=\"text-align: right;\">             3.5859 </td><td style=\"text-align: right;\">           1.16392</td><td style=\"text-align: right;\">       3.5859 </td><td style=\"text-align: right;\"> 1718904744</td><td style=\"text-align: right;\">                   2</td><td>fe339_00021</td></tr>\n",
       "<tr><td>train_fnn_fe339_00022</td><td style=\"text-align: right;\">       1.65457 </td><td style=\"text-align: right;\">       1.0015   </td><td>                     </td><td>2024-06-20_10-32-25</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         2</td><td>171.67.96.198</td><td style=\"text-align: right;\">605068</td><td style=\"text-align: right;\">             3.64204</td><td style=\"text-align: right;\">           1.19809</td><td style=\"text-align: right;\">       3.64204</td><td style=\"text-align: right;\"> 1718904745</td><td style=\"text-align: right;\">                   2</td><td>fe339_00022</td></tr>\n",
       "<tr><td>train_fnn_fe339_00023</td><td style=\"text-align: right;\">       6.76045 </td><td style=\"text-align: right;\">       4.99009  </td><td>                     </td><td>2024-06-20_10-32-30</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">605806</td><td style=\"text-align: right;\">             2.49208</td><td style=\"text-align: right;\">           2.49208</td><td style=\"text-align: right;\">       2.49208</td><td style=\"text-align: right;\"> 1718904750</td><td style=\"text-align: right;\">                   1</td><td>fe339_00023</td></tr>\n",
       "<tr><td>train_fnn_fe339_00024</td><td style=\"text-align: right;\">       0.117268</td><td style=\"text-align: right;\">       0.0933752</td><td>                     </td><td>2024-06-20_10-33-14</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                        30</td><td>171.67.96.198</td><td style=\"text-align: right;\">605807</td><td style=\"text-align: right;\">            46.8744 </td><td style=\"text-align: right;\">           1.51089</td><td style=\"text-align: right;\">      46.8744 </td><td style=\"text-align: right;\"> 1718904794</td><td style=\"text-align: right;\">                  30</td><td>fe339_00024</td></tr>\n",
       "<tr><td>train_fnn_fe339_00025</td><td style=\"text-align: right;\">       3.43273 </td><td style=\"text-align: right;\">       3.79597  </td><td>                     </td><td>2024-06-20_10-32-35</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         2</td><td>171.67.96.198</td><td style=\"text-align: right;\">606225</td><td style=\"text-align: right;\">             3.84809</td><td style=\"text-align: right;\">           1.32023</td><td style=\"text-align: right;\">       3.84809</td><td style=\"text-align: right;\"> 1718904755</td><td style=\"text-align: right;\">                   2</td><td>fe339_00025</td></tr>\n",
       "<tr><td>train_fnn_fe339_00026</td><td style=\"text-align: right;\">      67.3786  </td><td style=\"text-align: right;\">      70.4109   </td><td>                     </td><td>2024-06-20_10-32-33</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">606226</td><td style=\"text-align: right;\">             2.79193</td><td style=\"text-align: right;\">           2.79193</td><td style=\"text-align: right;\">       2.79193</td><td style=\"text-align: right;\"> 1718904753</td><td style=\"text-align: right;\">                   1</td><td>fe339_00026</td></tr>\n",
       "<tr><td>train_fnn_fe339_00027</td><td style=\"text-align: right;\">       0.469044</td><td style=\"text-align: right;\">       1.97782  </td><td>                     </td><td>2024-06-20_10-32-35</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         2</td><td>171.67.96.198</td><td style=\"text-align: right;\">606291</td><td style=\"text-align: right;\">             4.14984</td><td style=\"text-align: right;\">           1.43673</td><td style=\"text-align: right;\">       4.14984</td><td style=\"text-align: right;\"> 1718904755</td><td style=\"text-align: right;\">                   2</td><td>fe339_00027</td></tr>\n",
       "<tr><td>train_fnn_fe339_00028</td><td style=\"text-align: right;\">       0.144056</td><td style=\"text-align: right;\">       0.104458 </td><td>                     </td><td>2024-06-20_10-33-25</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                        30</td><td>171.67.96.198</td><td style=\"text-align: right;\">606912</td><td style=\"text-align: right;\">            49.2485 </td><td style=\"text-align: right;\">           1.58314</td><td style=\"text-align: right;\">      49.2485 </td><td style=\"text-align: right;\"> 1718904805</td><td style=\"text-align: right;\">                  30</td><td>fe339_00028</td></tr>\n",
       "<tr><td>train_fnn_fe339_00029</td><td style=\"text-align: right;\">       1.13399 </td><td style=\"text-align: right;\">       0.85156  </td><td>                     </td><td>2024-06-20_10-32-43</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         2</td><td>171.67.96.198</td><td style=\"text-align: right;\">609147</td><td style=\"text-align: right;\">             3.49647</td><td style=\"text-align: right;\">           1.1777 </td><td style=\"text-align: right;\">       3.49647</td><td style=\"text-align: right;\"> 1718904763</td><td style=\"text-align: right;\">                   2</td><td>fe339_00029</td></tr>\n",
       "<tr><td>train_fnn_fe339_00030</td><td style=\"text-align: right;\">       0.297692</td><td style=\"text-align: right;\">       0.351384 </td><td>                     </td><td>2024-06-20_10-32-49</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         4</td><td>171.67.96.198</td><td style=\"text-align: right;\">610530</td><td style=\"text-align: right;\">             8.2653 </td><td style=\"text-align: right;\">           1.77244</td><td style=\"text-align: right;\">       8.2653 </td><td style=\"text-align: right;\"> 1718904769</td><td style=\"text-align: right;\">                   4</td><td>fe339_00030</td></tr>\n",
       "<tr><td>train_fnn_fe339_00031</td><td style=\"text-align: right;\">      33.5616  </td><td style=\"text-align: right;\">      31.8923   </td><td>                     </td><td>2024-06-20_10-32-44</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">611299</td><td style=\"text-align: right;\">             2.6327 </td><td style=\"text-align: right;\">           2.6327 </td><td style=\"text-align: right;\">       2.6327 </td><td style=\"text-align: right;\"> 1718904764</td><td style=\"text-align: right;\">                   1</td><td>fe339_00031</td></tr>\n",
       "<tr><td>train_fnn_fe339_00032</td><td style=\"text-align: right;\">       6.67447 </td><td style=\"text-align: right;\">       9.03381  </td><td>                     </td><td>2024-06-20_10-32-48</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">615135</td><td style=\"text-align: right;\">             2.8482 </td><td style=\"text-align: right;\">           2.8482 </td><td style=\"text-align: right;\">       2.8482 </td><td style=\"text-align: right;\"> 1718904768</td><td style=\"text-align: right;\">                   1</td><td>fe339_00032</td></tr>\n",
       "<tr><td>train_fnn_fe339_00033</td><td style=\"text-align: right;\">       3.06844 </td><td style=\"text-align: right;\">       2.332    </td><td>                     </td><td>2024-06-20_10-32-50</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         2</td><td>171.67.96.198</td><td style=\"text-align: right;\">616500</td><td style=\"text-align: right;\">             3.56154</td><td style=\"text-align: right;\">           1.18669</td><td style=\"text-align: right;\">       3.56154</td><td style=\"text-align: right;\"> 1718904770</td><td style=\"text-align: right;\">                   2</td><td>fe339_00033</td></tr>\n",
       "<tr><td>train_fnn_fe339_00034</td><td style=\"text-align: right;\">      29.1474  </td><td style=\"text-align: right;\">      69.0866   </td><td>                     </td><td>2024-06-20_10-32-51</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">618382</td><td style=\"text-align: right;\">             2.64822</td><td style=\"text-align: right;\">           2.64822</td><td style=\"text-align: right;\">       2.64822</td><td style=\"text-align: right;\"> 1718904771</td><td style=\"text-align: right;\">                   1</td><td>fe339_00034</td></tr>\n",
       "<tr><td>train_fnn_fe339_00035</td><td style=\"text-align: right;\">       0.209653</td><td style=\"text-align: right;\">       0.156487 </td><td>                     </td><td>2024-06-20_10-32-57</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         4</td><td>171.67.96.198</td><td style=\"text-align: right;\">619131</td><td style=\"text-align: right;\">             7.83877</td><td style=\"text-align: right;\">           1.63513</td><td style=\"text-align: right;\">       7.83877</td><td style=\"text-align: right;\"> 1718904777</td><td style=\"text-align: right;\">                   4</td><td>fe339_00035</td></tr>\n",
       "<tr><td>train_fnn_fe339_00036</td><td style=\"text-align: right;\">       0.116714</td><td style=\"text-align: right;\">       0.0933149</td><td>                     </td><td>2024-06-20_10-33-30</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                        30</td><td>171.67.96.198</td><td style=\"text-align: right;\">623397</td><td style=\"text-align: right;\">            36.3969 </td><td style=\"text-align: right;\">           1.15925</td><td style=\"text-align: right;\">      36.3969 </td><td style=\"text-align: right;\"> 1718904810</td><td style=\"text-align: right;\">                  30</td><td>fe339_00036</td></tr>\n",
       "<tr><td>train_fnn_fe339_00037</td><td style=\"text-align: right;\">      16.2664  </td><td style=\"text-align: right;\">      11.5607   </td><td>                     </td><td>2024-06-20_10-32-56</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">623406</td><td style=\"text-align: right;\">             2.45173</td><td style=\"text-align: right;\">           2.45173</td><td style=\"text-align: right;\">       2.45173</td><td style=\"text-align: right;\"> 1718904776</td><td style=\"text-align: right;\">                   1</td><td>fe339_00037</td></tr>\n",
       "<tr><td>train_fnn_fe339_00038</td><td style=\"text-align: right;\">       5.79955 </td><td style=\"text-align: right;\">       5.69264  </td><td>                     </td><td>2024-06-20_10-32-57</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         1</td><td>171.67.96.198</td><td style=\"text-align: right;\">624755</td><td style=\"text-align: right;\">             2.46225</td><td style=\"text-align: right;\">           2.46225</td><td style=\"text-align: right;\">       2.46225</td><td style=\"text-align: right;\"> 1718904777</td><td style=\"text-align: right;\">                   1</td><td>fe339_00038</td></tr>\n",
       "<tr><td>train_fnn_fe339_00039</td><td style=\"text-align: right;\">       0.639259</td><td style=\"text-align: right;\">       0.439289 </td><td>                     </td><td>2024-06-20_10-32-59</td><td>True  </td><td>yen4      </td><td style=\"text-align: right;\">                         2</td><td>171.67.96.198</td><td style=\"text-align: right;\">624765</td><td style=\"text-align: right;\">             3.85822</td><td style=\"text-align: right;\">           1.28334</td><td style=\"text-align: right;\">       3.85822</td><td style=\"text-align: right;\"> 1718904779</td><td style=\"text-align: right;\">                   2</td><td>fe339_00039</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-06-20 10:32:07 (running for 00:00:10.37)\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.38412465973227633 | Iter 1.000: -6.925628302923215\n",
      "Logical resource usage: 9.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (30 PENDING, 5 RUNNING, 5 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00001 | RUNNING    | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        1.88971  |                    1 |\n",
      "| train_fnn_fe339_00003 | RUNNING    | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |        0.802797 |                    1 |\n",
      "| train_fnn_fe339_00004 | RUNNING    | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.192185 |                    2 |\n",
      "| train_fnn_fe339_00008 | RUNNING    | 171.67.96.198:601621 |      0.422551  |              16 |          100 | 0.00486964  |            2 |    1.8428e-06  |        0.384125 |                    2 |\n",
      "| train_fnn_fe339_00009 | RUNNING    | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        1.13028  |                    2 |\n",
      "| train_fnn_fe339_00010 | PENDING    |                      |      0.0207891 |               1 |          130 | 4.93862e-05 |            1 |    1.52983e-05 |                 |                      |\n",
      "| train_fnn_fe339_00011 | PENDING    |                      |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |                 |                      |\n",
      "| train_fnn_fe339_00012 | PENDING    |                      |      0.440076  |              41 |           90 | 2.10665e-05 |            3 |    1.55123e-06 |                 |                      |\n",
      "| train_fnn_fe339_00013 | PENDING    |                      |      0.199558  |              36 |           50 | 0.000175782 |            3 |    1.40858e-05 |                 |                      |\n",
      "| train_fnn_fe339_00014 | PENDING    |                      |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |                 |                      |\n",
      "| train_fnn_fe339_00015 | PENDING    |                      |      0.313142  |              46 |          150 | 6.41028e-06 |            2 |    0.000486733 |                 |                      |\n",
      "| train_fnn_fe339_00016 | PENDING    |                      |      0.182012  |              21 |           40 | 4.38058e-05 |            2 |    0.000184779 |                 |                      |\n",
      "| train_fnn_fe339_00017 | PENDING    |                      |      0.121243  |              11 |           70 | 0.00523348  |            3 |    0.000265681 |                 |                      |\n",
      "| train_fnn_fe339_00018 | PENDING    |                      |      0.0942407 |              46 |           60 | 2.6619e-05  |            2 |    0.000525394 |                 |                      |\n",
      "| train_fnn_fe339_00019 | PENDING    |                      |      0.220299  |              41 |          200 | 4.55642e-06 |            1 |    4.00192e-05 |                 |                      |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (20 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:32:12 (running for 00:00:15.40)\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -0.5770092552491263 | Iter 2.000: -0.4705592732803494 | Iter 1.000: -6.925628302923215\n",
      "Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (25 PENDING, 7 RUNNING, 8 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00004 | RUNNING    | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.318452 |                    6 |\n",
      "| train_fnn_fe339_00009 | RUNNING    | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.518594 |                    6 |\n",
      "| train_fnn_fe339_00010 | RUNNING    | 171.67.96.198:603302 |      0.0207891 |               1 |          130 | 4.93862e-05 |            1 |    1.52983e-05 |                 |                      |\n",
      "| train_fnn_fe339_00011 | RUNNING    | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |                 |                      |\n",
      "| train_fnn_fe339_00012 | RUNNING    | 171.67.96.198:603433 |      0.440076  |              41 |           90 | 2.10665e-05 |            3 |    1.55123e-06 |                 |                      |\n",
      "| train_fnn_fe339_00013 | RUNNING    | 171.67.96.198:603436 |      0.199558  |              36 |           50 | 0.000175782 |            3 |    1.40858e-05 |                 |                      |\n",
      "| train_fnn_fe339_00014 | RUNNING    | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |                 |                      |\n",
      "| train_fnn_fe339_00015 | PENDING    |                      |      0.313142  |              46 |          150 | 6.41028e-06 |            2 |    0.000486733 |                 |                      |\n",
      "| train_fnn_fe339_00016 | PENDING    |                      |      0.182012  |              21 |           40 | 4.38058e-05 |            2 |    0.000184779 |                 |                      |\n",
      "| train_fnn_fe339_00017 | PENDING    |                      |      0.121243  |              11 |           70 | 0.00523348  |            3 |    0.000265681 |                 |                      |\n",
      "| train_fnn_fe339_00018 | PENDING    |                      |      0.0942407 |              46 |           60 | 2.6619e-05  |            2 |    0.000525394 |                 |                      |\n",
      "| train_fnn_fe339_00019 | PENDING    |                      |      0.220299  |              41 |          200 | 4.55642e-06 |            1 |    4.00192e-05 |                 |                      |\n",
      "| train_fnn_fe339_00020 | PENDING    |                      |      0.242954  |              46 |           10 | 0.0059107   |            3 |    9.32414e-06 |                 |                      |\n",
      "| train_fnn_fe339_00021 | PENDING    |                      |      0.185202  |               1 |           90 | 9.71637e-06 |            1 |    0.000122719 |                 |                      |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (18 PENDING, 1 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:32:17 (running for 00:00:20.47)\n",
      "Using AsyncHyperBand: num_stopped=13\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.27293966985703294 | Iter 4.000: -0.5770092552491263 | Iter 2.000: -0.38412465973227633 | Iter 1.000: -8.087911848928414\n",
      "Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (22 PENDING, 5 RUNNING, 13 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00004 | RUNNING    | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.250805 |                   10 |\n",
      "| train_fnn_fe339_00009 | RUNNING    | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.271671 |                   10 |\n",
      "| train_fnn_fe339_00011 | RUNNING    | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.174321 |                    2 |\n",
      "| train_fnn_fe339_00014 | RUNNING    | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.168326 |                    3 |\n",
      "| train_fnn_fe339_00017 | RUNNING    | 171.67.96.198:604000 |      0.121243  |              11 |           70 | 0.00523348  |            3 |    0.000265681 |                 |                      |\n",
      "| train_fnn_fe339_00018 | PENDING    |                      |      0.0942407 |              46 |           60 | 2.6619e-05  |            2 |    0.000525394 |                 |                      |\n",
      "| train_fnn_fe339_00019 | PENDING    |                      |      0.220299  |              41 |          200 | 4.55642e-06 |            1 |    4.00192e-05 |                 |                      |\n",
      "| train_fnn_fe339_00020 | PENDING    |                      |      0.242954  |              46 |           10 | 0.0059107   |            3 |    9.32414e-06 |                 |                      |\n",
      "| train_fnn_fe339_00021 | PENDING    |                      |      0.185202  |               1 |           90 | 9.71637e-06 |            1 |    0.000122719 |                 |                      |\n",
      "| train_fnn_fe339_00022 | PENDING    |                      |      0.035452  |              36 |           50 | 0.00027432  |            1 |    3.22134e-05 |                 |                      |\n",
      "| train_fnn_fe339_00023 | PENDING    |                      |      0.175298  |               1 |           50 | 3.79821e-06 |            1 |    2.94007e-05 |                 |                      |\n",
      "| train_fnn_fe339_00024 | PENDING    |                      |      0.17397   |              36 |           40 | 0.000818211 |            3 |    1.26867e-05 |                 |                      |\n",
      "| train_fnn_fe339_00025 | PENDING    |                      |      0.113995  |               1 |           60 | 0.000107844 |            2 |    0.000122303 |                 |                      |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "| train_fnn_fe339_00008 | TERMINATED | 171.67.96.198:601621 |      0.422551  |              16 |          100 | 0.00486964  |            2 |    1.8428e-06  |        0.577009 |                    4 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (14 PENDING, 5 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:32:22 (running for 00:00:25.47)\n",
      "Using AsyncHyperBand: num_stopped=14\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.27293966985703294 | Iter 4.000: -0.20539618882478453 | Iter 2.000: -0.2939194055651528 | Iter 1.000: -5.890937041613012\n",
      "Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (17 PENDING, 9 RUNNING, 14 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00004 | RUNNING    | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.184841 |                   13 |\n",
      "| train_fnn_fe339_00009 | RUNNING    | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.14488  |                   15 |\n",
      "| train_fnn_fe339_00011 | RUNNING    | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.157604 |                    5 |\n",
      "| train_fnn_fe339_00014 | RUNNING    | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.123276 |                    7 |\n",
      "| train_fnn_fe339_00017 | RUNNING    | 171.67.96.198:604000 |      0.121243  |              11 |           70 | 0.00523348  |            3 |    0.000265681 |        0.146971 |                    3 |\n",
      "| train_fnn_fe339_00018 | RUNNING    | 171.67.96.198:604767 |      0.0942407 |              46 |           60 | 2.6619e-05  |            2 |    0.000525394 |                 |                      |\n",
      "| train_fnn_fe339_00020 | RUNNING    | 171.67.96.198:604945 |      0.242954  |              46 |           10 | 0.0059107   |            3 |    9.32414e-06 |                 |                      |\n",
      "| train_fnn_fe339_00023 | PENDING    |                      |      0.175298  |               1 |           50 | 3.79821e-06 |            1 |    2.94007e-05 |                 |                      |\n",
      "| train_fnn_fe339_00024 | PENDING    |                      |      0.17397   |              36 |           40 | 0.000818211 |            3 |    1.26867e-05 |                 |                      |\n",
      "| train_fnn_fe339_00025 | PENDING    |                      |      0.113995  |               1 |           60 | 0.000107844 |            2 |    0.000122303 |                 |                      |\n",
      "| train_fnn_fe339_00026 | PENDING    |                      |      0.363344  |              46 |           40 | 8.05347e-06 |            3 |    8.62022e-05 |                 |                      |\n",
      "| train_fnn_fe339_00027 | PENDING    |                      |      0.58802   |               6 |           10 | 0.000504383 |            3 |    0.000160564 |                 |                      |\n",
      "| train_fnn_fe339_00028 | PENDING    |                      |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |                 |                      |\n",
      "| train_fnn_fe339_00029 | PENDING    |                      |      0.631288  |              21 |           20 | 0.000340726 |            1 |    1.04013e-05 |                 |                      |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (2 RUNNING, 10 PENDING, 7 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:32:27 (running for 00:00:30.49)\n",
      "Using AsyncHyperBand: num_stopped=18\n",
      "Bracket: Iter 16.000: -0.17299997889333302 | Iter 8.000: -0.1704477708226715 | Iter 4.000: -0.20307205166984227 | Iter 2.000: -0.38412465973227633 | Iter 1.000: -5.3659566274655415\n",
      "Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (17 PENDING, 5 RUNNING, 18 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00009 | RUNNING    | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.126112 |                   19 |\n",
      "| train_fnn_fe339_00011 | RUNNING    | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.139103 |                    9 |\n",
      "| train_fnn_fe339_00014 | RUNNING    | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.119951 |                   12 |\n",
      "| train_fnn_fe339_00017 | RUNNING    | 171.67.96.198:604000 |      0.121243  |              11 |           70 | 0.00523348  |            3 |    0.000265681 |        0.145236 |                    6 |\n",
      "| train_fnn_fe339_00020 | RUNNING    | 171.67.96.198:604945 |      0.242954  |              46 |           10 | 0.0059107   |            3 |    9.32414e-06 |        0.116932 |                    3 |\n",
      "| train_fnn_fe339_00023 | PENDING    |                      |      0.175298  |               1 |           50 | 3.79821e-06 |            1 |    2.94007e-05 |                 |                      |\n",
      "| train_fnn_fe339_00024 | PENDING    |                      |      0.17397   |              36 |           40 | 0.000818211 |            3 |    1.26867e-05 |                 |                      |\n",
      "| train_fnn_fe339_00025 | PENDING    |                      |      0.113995  |               1 |           60 | 0.000107844 |            2 |    0.000122303 |                 |                      |\n",
      "| train_fnn_fe339_00026 | PENDING    |                      |      0.363344  |              46 |           40 | 8.05347e-06 |            3 |    8.62022e-05 |                 |                      |\n",
      "| train_fnn_fe339_00027 | PENDING    |                      |      0.58802   |               6 |           10 | 0.000504383 |            3 |    0.000160564 |                 |                      |\n",
      "| train_fnn_fe339_00028 | PENDING    |                      |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |                 |                      |\n",
      "| train_fnn_fe339_00029 | PENDING    |                      |      0.631288  |              21 |           20 | 0.000340726 |            1 |    1.04013e-05 |                 |                      |\n",
      "| train_fnn_fe339_00030 | PENDING    |                      |      0.62209   |              16 |          160 | 0.00131675  |            3 |    8.43519e-05 |                 |                      |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (9 PENDING, 10 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:32:32 (running for 00:00:35.54)\n",
      "Using AsyncHyperBand: num_stopped=19\n",
      "Bracket: Iter 16.000: -0.14718003337289773 | Iter 8.000: -0.15999592679972743 | Iter 4.000: -0.20074791451490004 | Iter 2.000: -0.2939194055651528 | Iter 1.000: -5.3659566274655415\n",
      "Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (12 PENDING, 9 RUNNING, 19 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00009 | RUNNING    | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.149216 |                   23 |\n",
      "| train_fnn_fe339_00011 | RUNNING    | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.131718 |                   12 |\n",
      "| train_fnn_fe339_00014 | RUNNING    | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.122083 |                   16 |\n",
      "| train_fnn_fe339_00017 | RUNNING    | 171.67.96.198:604000 |      0.121243  |              11 |           70 | 0.00523348  |            3 |    0.000265681 |        0.208048 |                   10 |\n",
      "| train_fnn_fe339_00020 | RUNNING    | 171.67.96.198:604945 |      0.242954  |              46 |           10 | 0.0059107   |            3 |    9.32414e-06 |        0.120114 |                    6 |\n",
      "| train_fnn_fe339_00024 | RUNNING    | 171.67.96.198:605807 |      0.17397   |              36 |           40 | 0.000818211 |            3 |    1.26867e-05 |        0.1491   |                    2 |\n",
      "| train_fnn_fe339_00025 | RUNNING    | 171.67.96.198:606225 |      0.113995  |               1 |           60 | 0.000107844 |            2 |    0.000122303 |                 |                      |\n",
      "| train_fnn_fe339_00028 | PENDING    |                      |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |                 |                      |\n",
      "| train_fnn_fe339_00029 | PENDING    |                      |      0.631288  |              21 |           20 | 0.000340726 |            1 |    1.04013e-05 |                 |                      |\n",
      "| train_fnn_fe339_00030 | PENDING    |                      |      0.62209   |              16 |          160 | 0.00131675  |            3 |    8.43519e-05 |                 |                      |\n",
      "| train_fnn_fe339_00031 | PENDING    |                      |      0.428436  |              36 |           80 | 1.0884e-06  |            2 |    2.01565e-06 |                 |                      |\n",
      "| train_fnn_fe339_00032 | PENDING    |                      |      0.345073  |               6 |          180 | 6.21866e-05 |            2 |    0.000962437 |                 |                      |\n",
      "| train_fnn_fe339_00033 | PENDING    |                      |      0.133388  |              21 |           40 | 2.92331e-05 |            1 |    0.000170809 |                 |                      |\n",
      "| train_fnn_fe339_00034 | PENDING    |                      |      0.44907   |              11 |           10 | 1.00426e-05 |            3 |    5.88319e-05 |                 |                      |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (2 RUNNING, 5 PENDING, 12 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:32:37 (running for 00:00:40.58)\n",
      "Using AsyncHyperBand: num_stopped=22\n",
      "Bracket: Iter 16.000: -0.14718003337289773 | Iter 8.000: -0.14213916403697988 | Iter 4.000: -0.17171807385055848 | Iter 2.000: -0.4265841652267899 | Iter 1.000: -5.1404406432232825\n",
      "Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (11 PENDING, 7 RUNNING, 22 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00009 | RUNNING    | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.120827 |                   27 |\n",
      "| train_fnn_fe339_00011 | RUNNING    | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.138756 |                   15 |\n",
      "| train_fnn_fe339_00014 | RUNNING    | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.125008 |                   20 |\n",
      "| train_fnn_fe339_00017 | RUNNING    | 171.67.96.198:604000 |      0.121243  |              11 |           70 | 0.00523348  |            3 |    0.000265681 |        0.119993 |                   13 |\n",
      "| train_fnn_fe339_00020 | RUNNING    | 171.67.96.198:604945 |      0.242954  |              46 |           10 | 0.0059107   |            3 |    9.32414e-06 |        0.116137 |                   10 |\n",
      "| train_fnn_fe339_00024 | RUNNING    | 171.67.96.198:605807 |      0.17397   |              36 |           40 | 0.000818211 |            3 |    1.26867e-05 |        0.135329 |                    5 |\n",
      "| train_fnn_fe339_00028 | RUNNING    | 171.67.96.198:606912 |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |                 |                      |\n",
      "| train_fnn_fe339_00029 | PENDING    |                      |      0.631288  |              21 |           20 | 0.000340726 |            1 |    1.04013e-05 |                 |                      |\n",
      "| train_fnn_fe339_00030 | PENDING    |                      |      0.62209   |              16 |          160 | 0.00131675  |            3 |    8.43519e-05 |                 |                      |\n",
      "| train_fnn_fe339_00031 | PENDING    |                      |      0.428436  |              36 |           80 | 1.0884e-06  |            2 |    2.01565e-06 |                 |                      |\n",
      "| train_fnn_fe339_00032 | PENDING    |                      |      0.345073  |               6 |          180 | 6.21866e-05 |            2 |    0.000962437 |                 |                      |\n",
      "| train_fnn_fe339_00033 | PENDING    |                      |      0.133388  |              21 |           40 | 2.92331e-05 |            1 |    0.000170809 |                 |                      |\n",
      "| train_fnn_fe339_00034 | PENDING    |                      |      0.44907   |              11 |           10 | 1.00426e-05 |            3 |    5.88319e-05 |                 |                      |\n",
      "| train_fnn_fe339_00035 | PENDING    |                      |      0.445486  |              36 |          190 | 0.00151093  |            2 |    3.22041e-05 |                 |                      |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (4 PENDING, 15 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:32:42 (running for 00:00:45.64)\n",
      "Using AsyncHyperBand: num_stopped=24\n",
      "Bracket: Iter 16.000: -0.14718003337289773 | Iter 8.000: -0.13356392341209392 | Iter 4.000: -0.17171807385055848 | Iter 2.000: -0.38412465973227633 | Iter 1.000: -3.4943539675544293\n",
      "Logical resource usage: 9.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (8 PENDING, 8 RUNNING, 24 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00011 | RUNNING    | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.131523 |                   18 |\n",
      "| train_fnn_fe339_00014 | RUNNING    | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.122695 |                   25 |\n",
      "| train_fnn_fe339_00020 | RUNNING    | 171.67.96.198:604945 |      0.242954  |              46 |           10 | 0.0059107   |            3 |    9.32414e-06 |        0.115613 |                   13 |\n",
      "| train_fnn_fe339_00024 | RUNNING    | 171.67.96.198:605807 |      0.17397   |              36 |           40 | 0.000818211 |            3 |    1.26867e-05 |        0.133564 |                    8 |\n",
      "| train_fnn_fe339_00028 | RUNNING    | 171.67.96.198:606912 |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |        0.164559 |                    3 |\n",
      "| train_fnn_fe339_00029 | RUNNING    | 171.67.96.198:609147 |      0.631288  |              21 |           20 | 0.000340726 |            1 |    1.04013e-05 |        1.76325  |                    1 |\n",
      "| train_fnn_fe339_00030 | RUNNING    | 171.67.96.198:610530 |      0.62209   |              16 |          160 | 0.00131675  |            3 |    8.43519e-05 |                 |                      |\n",
      "| train_fnn_fe339_00032 | PENDING    |                      |      0.345073  |               6 |          180 | 6.21866e-05 |            2 |    0.000962437 |                 |                      |\n",
      "| train_fnn_fe339_00033 | PENDING    |                      |      0.133388  |              21 |           40 | 2.92331e-05 |            1 |    0.000170809 |                 |                      |\n",
      "| train_fnn_fe339_00034 | PENDING    |                      |      0.44907   |              11 |           10 | 1.00426e-05 |            3 |    5.88319e-05 |                 |                      |\n",
      "| train_fnn_fe339_00035 | PENDING    |                      |      0.445486  |              36 |          190 | 0.00151093  |            2 |    3.22041e-05 |                 |                      |\n",
      "| train_fnn_fe339_00036 | PENDING    |                      |      0.404683  |              11 |           10 | 0.00118636  |            1 |    1.35149e-06 |                 |                      |\n",
      "| train_fnn_fe339_00037 | PENDING    |                      |      0.26541   |              26 |          140 | 1.15299e-06 |            1 |    0.000609475 |                 |                      |\n",
      "| train_fnn_fe339_00038 | PENDING    |                      |      0.21317   |              16 |           30 | 3.47049e-05 |            1 |    0.00035761  |                 |                      |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (1 RUNNING, 1 PENDING, 17 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:32:47 (running for 00:00:50.64)\n",
      "Using AsyncHyperBand: num_stopped=26\n",
      "Bracket: Iter 16.000: -0.13655697747918905 | Iter 8.000: -0.13356392341209392 | Iter 4.000: -0.14268823318621693 | Iter 2.000: -0.38412465973227633 | Iter 1.000: -3.4943539675544293\n",
      "Logical resource usage: 10.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (6 PENDING, 8 RUNNING, 26 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00011 | RUNNING    | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.183348 |                   21 |\n",
      "| train_fnn_fe339_00014 | RUNNING    | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.13026  |                   29 |\n",
      "| train_fnn_fe339_00020 | RUNNING    | 171.67.96.198:604945 |      0.242954  |              46 |           10 | 0.0059107   |            3 |    9.32414e-06 |        0.11488  |                   16 |\n",
      "| train_fnn_fe339_00024 | RUNNING    | 171.67.96.198:605807 |      0.17397   |              36 |           40 | 0.000818211 |            3 |    1.26867e-05 |        0.118766 |                   12 |\n",
      "| train_fnn_fe339_00028 | RUNNING    | 171.67.96.198:606912 |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |        0.133501 |                    6 |\n",
      "| train_fnn_fe339_00030 | RUNNING    | 171.67.96.198:610530 |      0.62209   |              16 |          160 | 0.00131675  |            3 |    8.43519e-05 |        0.208005 |                    3 |\n",
      "| train_fnn_fe339_00032 | RUNNING    | 171.67.96.198:615135 |      0.345073  |               6 |          180 | 6.21866e-05 |            2 |    0.000962437 |                 |                      |\n",
      "| train_fnn_fe339_00034 | PENDING    |                      |      0.44907   |              11 |           10 | 1.00426e-05 |            3 |    5.88319e-05 |                 |                      |\n",
      "| train_fnn_fe339_00035 | PENDING    |                      |      0.445486  |              36 |          190 | 0.00151093  |            2 |    3.22041e-05 |                 |                      |\n",
      "| train_fnn_fe339_00036 | PENDING    |                      |      0.404683  |              11 |           10 | 0.00118636  |            1 |    1.35149e-06 |                 |                      |\n",
      "| train_fnn_fe339_00037 | PENDING    |                      |      0.26541   |              26 |          140 | 1.15299e-06 |            1 |    0.000609475 |                 |                      |\n",
      "| train_fnn_fe339_00038 | PENDING    |                      |      0.21317   |              16 |           30 | 3.47049e-05 |            1 |    0.00035761  |                 |                      |\n",
      "| train_fnn_fe339_00039 | PENDING    |                      |      0.655947  |              46 |          180 | 0.000608302 |            1 |    5.13078e-05 |                 |                      |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (1 RUNNING, 19 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:32:52 (running for 00:00:55.67)\n",
      "Using AsyncHyperBand: num_stopped=31\n",
      "Bracket: Iter 16.000: -0.13655697747918905 | Iter 8.000: -0.12892316234316312 | Iter 4.000: -0.17171807385055848 | Iter 2.000: -0.4265841652267899 | Iter 1.000: -4.537167676913193\n",
      "Logical resource usage: 9.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (4 PENDING, 5 RUNNING, 31 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00011 | RUNNING    | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.257879 |                   24 |\n",
      "| train_fnn_fe339_00020 | RUNNING    | 171.67.96.198:604945 |      0.242954  |              46 |           10 | 0.0059107   |            3 |    9.32414e-06 |        0.136682 |                   20 |\n",
      "| train_fnn_fe339_00024 | RUNNING    | 171.67.96.198:605807 |      0.17397   |              36 |           40 | 0.000818211 |            3 |    1.26867e-05 |        0.126445 |                   15 |\n",
      "| train_fnn_fe339_00028 | RUNNING    | 171.67.96.198:606912 |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |        0.132538 |                    9 |\n",
      "| train_fnn_fe339_00035 | RUNNING    | 171.67.96.198:619131 |      0.445486  |              36 |          190 | 0.00151093  |            2 |    3.22041e-05 |        0.249076 |                    1 |\n",
      "| train_fnn_fe339_00036 | PENDING    |                      |      0.404683  |              11 |           10 | 0.00118636  |            1 |    1.35149e-06 |                 |                      |\n",
      "| train_fnn_fe339_00037 | PENDING    |                      |      0.26541   |              26 |          140 | 1.15299e-06 |            1 |    0.000609475 |                 |                      |\n",
      "| train_fnn_fe339_00038 | PENDING    |                      |      0.21317   |              16 |           30 | 3.47049e-05 |            1 |    0.00035761  |                 |                      |\n",
      "| train_fnn_fe339_00039 | PENDING    |                      |      0.655947  |              46 |          180 | 0.000608302 |            1 |    5.13078e-05 |                 |                      |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "| train_fnn_fe339_00008 | TERMINATED | 171.67.96.198:601621 |      0.422551  |              16 |          100 | 0.00486964  |            2 |    1.8428e-06  |        0.577009 |                    4 |\n",
      "| train_fnn_fe339_00009 | TERMINATED | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.119962 |                   30 |\n",
      "| train_fnn_fe339_00010 | TERMINATED | 171.67.96.198:603302 |      0.0207891 |               1 |          130 | 4.93862e-05 |            1 |    1.52983e-05 |        1.90933  |                    1 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (20 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:32:57 (running for 00:01:00.77)\n",
      "Using AsyncHyperBand: num_stopped=34\n",
      "Bracket: Iter 16.000: -0.13397791645690507 | Iter 8.000: -0.12892316234316312 | Iter 4.000: -0.20074791451490004 | Iter 2.000: -0.3514833977701617 | Iter 1.000: -4.537167676913193\n",
      "Logical resource usage: 6.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (6 RUNNING, 34 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00011 | RUNNING    | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.141555 |                   27 |\n",
      "| train_fnn_fe339_00020 | RUNNING    | 171.67.96.198:604945 |      0.242954  |              46 |           10 | 0.0059107   |            3 |    9.32414e-06 |        0.118086 |                   23 |\n",
      "| train_fnn_fe339_00024 | RUNNING    | 171.67.96.198:605807 |      0.17397   |              36 |           40 | 0.000818211 |            3 |    1.26867e-05 |        0.126239 |                   18 |\n",
      "| train_fnn_fe339_00028 | RUNNING    | 171.67.96.198:606912 |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |        0.145275 |                   13 |\n",
      "| train_fnn_fe339_00036 | RUNNING    | 171.67.96.198:623397 |      0.404683  |              11 |           10 | 0.00118636  |            1 |    1.35149e-06 |        0.318842 |                    2 |\n",
      "| train_fnn_fe339_00039 | RUNNING    | 171.67.96.198:624765 |      0.655947  |              46 |          180 | 0.000608302 |            1 |    5.13078e-05 |        1.1389   |                    1 |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "| train_fnn_fe339_00008 | TERMINATED | 171.67.96.198:601621 |      0.422551  |              16 |          100 | 0.00486964  |            2 |    1.8428e-06  |        0.577009 |                    4 |\n",
      "| train_fnn_fe339_00009 | TERMINATED | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.119962 |                   30 |\n",
      "| train_fnn_fe339_00010 | TERMINATED | 171.67.96.198:603302 |      0.0207891 |               1 |          130 | 4.93862e-05 |            1 |    1.52983e-05 |        1.90933  |                    1 |\n",
      "| train_fnn_fe339_00012 | TERMINATED | 171.67.96.198:603433 |      0.440076  |              41 |           90 | 2.10665e-05 |            3 |    1.55123e-06 |       43.4836   |                    1 |\n",
      "| train_fnn_fe339_00013 | TERMINATED | 171.67.96.198:603436 |      0.199558  |              36 |           50 | 0.000175782 |            3 |    1.40858e-05 |        8.08791  |                    1 |\n",
      "| train_fnn_fe339_00014 | TERMINATED | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.133924 |                   30 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (20 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:33:03 (running for 00:01:05.83)\n",
      "Using AsyncHyperBand: num_stopped=36\n",
      "Bracket: Iter 16.000: -0.1299675765380361 | Iter 8.000: -0.12892316234316312 | Iter 4.000: -0.17171807385055848 | Iter 2.000: -0.38412465973227633 | Iter 1.000: -4.537167676913193\n",
      "Logical resource usage: 4.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (4 RUNNING, 36 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00020 | RUNNING    | 171.67.96.198:604945 |      0.242954  |              46 |           10 | 0.0059107   |            3 |    9.32414e-06 |        0.116482 |                   27 |\n",
      "| train_fnn_fe339_00024 | RUNNING    | 171.67.96.198:605807 |      0.17397   |              36 |           40 | 0.000818211 |            3 |    1.26867e-05 |        0.116087 |                   22 |\n",
      "| train_fnn_fe339_00028 | RUNNING    | 171.67.96.198:606912 |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |        0.125957 |                   16 |\n",
      "| train_fnn_fe339_00036 | RUNNING    | 171.67.96.198:623397 |      0.404683  |              11 |           10 | 0.00118636  |            1 |    1.35149e-06 |        0.119133 |                    6 |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "| train_fnn_fe339_00008 | TERMINATED | 171.67.96.198:601621 |      0.422551  |              16 |          100 | 0.00486964  |            2 |    1.8428e-06  |        0.577009 |                    4 |\n",
      "| train_fnn_fe339_00009 | TERMINATED | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.119962 |                   30 |\n",
      "| train_fnn_fe339_00010 | TERMINATED | 171.67.96.198:603302 |      0.0207891 |               1 |          130 | 4.93862e-05 |            1 |    1.52983e-05 |        1.90933  |                    1 |\n",
      "| train_fnn_fe339_00011 | TERMINATED | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.120435 |                   30 |\n",
      "| train_fnn_fe339_00012 | TERMINATED | 171.67.96.198:603433 |      0.440076  |              41 |           90 | 2.10665e-05 |            3 |    1.55123e-06 |       43.4836   |                    1 |\n",
      "| train_fnn_fe339_00013 | TERMINATED | 171.67.96.198:603436 |      0.199558  |              36 |           50 | 0.000175782 |            3 |    1.40858e-05 |        8.08791  |                    1 |\n",
      "| train_fnn_fe339_00014 | TERMINATED | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.133924 |                   30 |\n",
      "| train_fnn_fe339_00015 | TERMINATED | 171.67.96.198:603772 |      0.313142  |              46 |          150 | 6.41028e-06 |            2 |    0.000486733 |       14.2128   |                    1 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (20 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:33:08 (running for 00:01:10.92)\n",
      "Using AsyncHyperBand: num_stopped=37\n",
      "Bracket: Iter 16.000: -0.1299675765380361 | Iter 8.000: -0.1242824012742323 | Iter 4.000: -0.17171807385055848 | Iter 2.000: -0.38412465973227633 | Iter 1.000: -4.537167676913193\n",
      "Logical resource usage: 3.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (3 RUNNING, 37 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00024 | RUNNING    | 171.67.96.198:605807 |      0.17397   |              36 |           40 | 0.000818211 |            3 |    1.26867e-05 |        0.118235 |                   25 |\n",
      "| train_fnn_fe339_00028 | RUNNING    | 171.67.96.198:606912 |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |        0.191909 |                   19 |\n",
      "| train_fnn_fe339_00036 | RUNNING    | 171.67.96.198:623397 |      0.404683  |              11 |           10 | 0.00118636  |            1 |    1.35149e-06 |        0.119285 |                   10 |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "| train_fnn_fe339_00008 | TERMINATED | 171.67.96.198:601621 |      0.422551  |              16 |          100 | 0.00486964  |            2 |    1.8428e-06  |        0.577009 |                    4 |\n",
      "| train_fnn_fe339_00009 | TERMINATED | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.119962 |                   30 |\n",
      "| train_fnn_fe339_00010 | TERMINATED | 171.67.96.198:603302 |      0.0207891 |               1 |          130 | 4.93862e-05 |            1 |    1.52983e-05 |        1.90933  |                    1 |\n",
      "| train_fnn_fe339_00011 | TERMINATED | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.120435 |                   30 |\n",
      "| train_fnn_fe339_00012 | TERMINATED | 171.67.96.198:603433 |      0.440076  |              41 |           90 | 2.10665e-05 |            3 |    1.55123e-06 |       43.4836   |                    1 |\n",
      "| train_fnn_fe339_00013 | TERMINATED | 171.67.96.198:603436 |      0.199558  |              36 |           50 | 0.000175782 |            3 |    1.40858e-05 |        8.08791  |                    1 |\n",
      "| train_fnn_fe339_00014 | TERMINATED | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.133924 |                   30 |\n",
      "| train_fnn_fe339_00015 | TERMINATED | 171.67.96.198:603772 |      0.313142  |              46 |          150 | 6.41028e-06 |            2 |    0.000486733 |       14.2128   |                    1 |\n",
      "| train_fnn_fe339_00016 | TERMINATED | 171.67.96.198:603773 |      0.182012  |              21 |           40 | 4.38058e-05 |            2 |    0.000184779 |       10.5261   |                    1 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (20 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:33:13 (running for 00:01:16.02)\n",
      "Using AsyncHyperBand: num_stopped=37\n",
      "Bracket: Iter 16.000: -0.1299675765380361 | Iter 8.000: -0.1242824012742323 | Iter 4.000: -0.17171807385055848 | Iter 2.000: -0.38412465973227633 | Iter 1.000: -4.537167676913193\n",
      "Logical resource usage: 3.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (3 RUNNING, 37 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00024 | RUNNING    | 171.67.96.198:605807 |      0.17397   |              36 |           40 | 0.000818211 |            3 |    1.26867e-05 |        0.117714 |                   28 |\n",
      "| train_fnn_fe339_00028 | RUNNING    | 171.67.96.198:606912 |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |        0.241784 |                   22 |\n",
      "| train_fnn_fe339_00036 | RUNNING    | 171.67.96.198:623397 |      0.404683  |              11 |           10 | 0.00118636  |            1 |    1.35149e-06 |        0.116341 |                   15 |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "| train_fnn_fe339_00008 | TERMINATED | 171.67.96.198:601621 |      0.422551  |              16 |          100 | 0.00486964  |            2 |    1.8428e-06  |        0.577009 |                    4 |\n",
      "| train_fnn_fe339_00009 | TERMINATED | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.119962 |                   30 |\n",
      "| train_fnn_fe339_00010 | TERMINATED | 171.67.96.198:603302 |      0.0207891 |               1 |          130 | 4.93862e-05 |            1 |    1.52983e-05 |        1.90933  |                    1 |\n",
      "| train_fnn_fe339_00011 | TERMINATED | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.120435 |                   30 |\n",
      "| train_fnn_fe339_00012 | TERMINATED | 171.67.96.198:603433 |      0.440076  |              41 |           90 | 2.10665e-05 |            3 |    1.55123e-06 |       43.4836   |                    1 |\n",
      "| train_fnn_fe339_00013 | TERMINATED | 171.67.96.198:603436 |      0.199558  |              36 |           50 | 0.000175782 |            3 |    1.40858e-05 |        8.08791  |                    1 |\n",
      "| train_fnn_fe339_00014 | TERMINATED | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.133924 |                   30 |\n",
      "| train_fnn_fe339_00015 | TERMINATED | 171.67.96.198:603772 |      0.313142  |              46 |          150 | 6.41028e-06 |            2 |    0.000486733 |       14.2128   |                    1 |\n",
      "| train_fnn_fe339_00016 | TERMINATED | 171.67.96.198:603773 |      0.182012  |              21 |           40 | 4.38058e-05 |            2 |    0.000184779 |       10.5261   |                    1 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (20 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:33:18 (running for 00:01:21.10)\n",
      "Using AsyncHyperBand: num_stopped=38\n",
      "Bracket: Iter 16.000: -0.12595723661916708 | Iter 8.000: -0.1242824012742323 | Iter 4.000: -0.17171807385055848 | Iter 2.000: -0.38412465973227633 | Iter 1.000: -4.537167676913193\n",
      "Logical resource usage: 2.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (2 RUNNING, 38 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00028 | RUNNING    | 171.67.96.198:606912 |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |        0.129275 |                   25 |\n",
      "| train_fnn_fe339_00036 | RUNNING    | 171.67.96.198:623397 |      0.404683  |              11 |           10 | 0.00118636  |            1 |    1.35149e-06 |        0.119084 |                   19 |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "| train_fnn_fe339_00008 | TERMINATED | 171.67.96.198:601621 |      0.422551  |              16 |          100 | 0.00486964  |            2 |    1.8428e-06  |        0.577009 |                    4 |\n",
      "| train_fnn_fe339_00009 | TERMINATED | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.119962 |                   30 |\n",
      "| train_fnn_fe339_00010 | TERMINATED | 171.67.96.198:603302 |      0.0207891 |               1 |          130 | 4.93862e-05 |            1 |    1.52983e-05 |        1.90933  |                    1 |\n",
      "| train_fnn_fe339_00011 | TERMINATED | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.120435 |                   30 |\n",
      "| train_fnn_fe339_00012 | TERMINATED | 171.67.96.198:603433 |      0.440076  |              41 |           90 | 2.10665e-05 |            3 |    1.55123e-06 |       43.4836   |                    1 |\n",
      "| train_fnn_fe339_00013 | TERMINATED | 171.67.96.198:603436 |      0.199558  |              36 |           50 | 0.000175782 |            3 |    1.40858e-05 |        8.08791  |                    1 |\n",
      "| train_fnn_fe339_00014 | TERMINATED | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.133924 |                   30 |\n",
      "| train_fnn_fe339_00015 | TERMINATED | 171.67.96.198:603772 |      0.313142  |              46 |          150 | 6.41028e-06 |            2 |    0.000486733 |       14.2128   |                    1 |\n",
      "| train_fnn_fe339_00016 | TERMINATED | 171.67.96.198:603773 |      0.182012  |              21 |           40 | 4.38058e-05 |            2 |    0.000184779 |       10.5261   |                    1 |\n",
      "| train_fnn_fe339_00017 | TERMINATED | 171.67.96.198:604000 |      0.121243  |              11 |           70 | 0.00523348  |            3 |    0.000265681 |        1.48382  |                   16 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (20 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:33:23 (running for 00:01:26.17)\n",
      "Using AsyncHyperBand: num_stopped=38\n",
      "Bracket: Iter 16.000: -0.12595723661916708 | Iter 8.000: -0.1242824012742323 | Iter 4.000: -0.17171807385055848 | Iter 2.000: -0.38412465973227633 | Iter 1.000: -4.537167676913193\n",
      "Logical resource usage: 2.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (2 RUNNING, 38 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00028 | RUNNING    | 171.67.96.198:606912 |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |        0.149531 |                   28 |\n",
      "| train_fnn_fe339_00036 | RUNNING    | 171.67.96.198:623397 |      0.404683  |              11 |           10 | 0.00118636  |            1 |    1.35149e-06 |        0.117538 |                   23 |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "| train_fnn_fe339_00008 | TERMINATED | 171.67.96.198:601621 |      0.422551  |              16 |          100 | 0.00486964  |            2 |    1.8428e-06  |        0.577009 |                    4 |\n",
      "| train_fnn_fe339_00009 | TERMINATED | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.119962 |                   30 |\n",
      "| train_fnn_fe339_00010 | TERMINATED | 171.67.96.198:603302 |      0.0207891 |               1 |          130 | 4.93862e-05 |            1 |    1.52983e-05 |        1.90933  |                    1 |\n",
      "| train_fnn_fe339_00011 | TERMINATED | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.120435 |                   30 |\n",
      "| train_fnn_fe339_00012 | TERMINATED | 171.67.96.198:603433 |      0.440076  |              41 |           90 | 2.10665e-05 |            3 |    1.55123e-06 |       43.4836   |                    1 |\n",
      "| train_fnn_fe339_00013 | TERMINATED | 171.67.96.198:603436 |      0.199558  |              36 |           50 | 0.000175782 |            3 |    1.40858e-05 |        8.08791  |                    1 |\n",
      "| train_fnn_fe339_00014 | TERMINATED | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.133924 |                   30 |\n",
      "| train_fnn_fe339_00015 | TERMINATED | 171.67.96.198:603772 |      0.313142  |              46 |          150 | 6.41028e-06 |            2 |    0.000486733 |       14.2128   |                    1 |\n",
      "| train_fnn_fe339_00016 | TERMINATED | 171.67.96.198:603773 |      0.182012  |              21 |           40 | 4.38058e-05 |            2 |    0.000184779 |       10.5261   |                    1 |\n",
      "| train_fnn_fe339_00017 | TERMINATED | 171.67.96.198:604000 |      0.121243  |              11 |           70 | 0.00523348  |            3 |    0.000265681 |        1.48382  |                   16 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (20 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-06-20 10:33:28 (running for 00:01:31.23)\n",
      "Using AsyncHyperBand: num_stopped=39\n",
      "Bracket: Iter 16.000: -0.12595723661916708 | Iter 8.000: -0.1242824012742323 | Iter 4.000: -0.17171807385055848 | Iter 2.000: -0.38412465973227633 | Iter 1.000: -4.537167676913193\n",
      "Logical resource usage: 1.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (1 RUNNING, 39 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00036 | RUNNING    | 171.67.96.198:623397 |      0.404683  |              11 |           10 | 0.00118636  |            1 |    1.35149e-06 |        0.124334 |                   28 |\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "| train_fnn_fe339_00008 | TERMINATED | 171.67.96.198:601621 |      0.422551  |              16 |          100 | 0.00486964  |            2 |    1.8428e-06  |        0.577009 |                    4 |\n",
      "| train_fnn_fe339_00009 | TERMINATED | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.119962 |                   30 |\n",
      "| train_fnn_fe339_00010 | TERMINATED | 171.67.96.198:603302 |      0.0207891 |               1 |          130 | 4.93862e-05 |            1 |    1.52983e-05 |        1.90933  |                    1 |\n",
      "| train_fnn_fe339_00011 | TERMINATED | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.120435 |                   30 |\n",
      "| train_fnn_fe339_00012 | TERMINATED | 171.67.96.198:603433 |      0.440076  |              41 |           90 | 2.10665e-05 |            3 |    1.55123e-06 |       43.4836   |                    1 |\n",
      "| train_fnn_fe339_00013 | TERMINATED | 171.67.96.198:603436 |      0.199558  |              36 |           50 | 0.000175782 |            3 |    1.40858e-05 |        8.08791  |                    1 |\n",
      "| train_fnn_fe339_00014 | TERMINATED | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.133924 |                   30 |\n",
      "| train_fnn_fe339_00015 | TERMINATED | 171.67.96.198:603772 |      0.313142  |              46 |          150 | 6.41028e-06 |            2 |    0.000486733 |       14.2128   |                    1 |\n",
      "| train_fnn_fe339_00016 | TERMINATED | 171.67.96.198:603773 |      0.182012  |              21 |           40 | 4.38058e-05 |            2 |    0.000184779 |       10.5261   |                    1 |\n",
      "| train_fnn_fe339_00017 | TERMINATED | 171.67.96.198:604000 |      0.121243  |              11 |           70 | 0.00523348  |            3 |    0.000265681 |        1.48382  |                   16 |\n",
      "| train_fnn_fe339_00018 | TERMINATED | 171.67.96.198:604767 |      0.0942407 |              46 |           60 | 2.6619e-05  |            2 |    0.000525394 |        9.85233  |                    1 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "... 20 more trials not shown (20 TERMINATED)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neural Network:Best trial config: {'input_dim': 102, 'hidden_dim': 10, 'num_layers': 3, 'num_embeddings': 953, 'embedding_dim': 46, 'dropout_rate': 0.24295446826850328, 'lr': 0.005910698619088546, 'weight_decay': 9.324140221663508e-06, 'num_epochs': 30, 'gpu': 0}\n",
      "INFO:Neural Network:Best trial training loss: 0.09510207540102747\n",
      "INFO:Neural Network:Best trial testing loss: 0.11536250937900512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-06-20 10:33:30 (running for 00:01:33.28)\n",
      "Using AsyncHyperBand: num_stopped=40\n",
      "Bracket: Iter 16.000: -0.12595723661916708 | Iter 8.000: -0.1242824012742323 | Iter 4.000: -0.17171807385055848 | Iter 2.000: -0.38412465973227633 | Iter 1.000: -4.537167676913193\n",
      "Logical resource usage: 1.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2024-06-20_10-31-52_670893_552272/artifacts/2024-06-20_10-31-57/train_fnn_2024-06-20_10-31-57/driver_artifacts\n",
      "Number of trials: 40/40 (40 TERMINATED)\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "| Trial name            | status     | loc                  |   dropout_rate |   embedding_dim |   hidden_dim |          lr |   num_layers |   weight_decay |   avg_test_loss |   training_iteration |\n",
      "|-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------|\n",
      "| train_fnn_fe339_00000 | TERMINATED | 171.67.96.198:601613 |      0.423074  |              36 |           70 | 4.20799e-06 |            1 |    2.93754e-06 |       11.9615   |                    1 |\n",
      "| train_fnn_fe339_00001 | TERMINATED | 171.67.96.198:601614 |      0.048924  |              26 |           40 | 0.000772672 |            3 |    0.000654121 |        0.470559 |                    2 |\n",
      "| train_fnn_fe339_00002 | TERMINATED | 171.67.96.198:601615 |      0.0148758 |              26 |           10 | 1.23666e-06 |            2 |    3.75253e-05 |       88.2948   |                    1 |\n",
      "| train_fnn_fe339_00003 | TERMINATED | 171.67.96.198:601616 |      0.273899  |              11 |          160 | 0.00856887  |            3 |    2.51356e-05 |      171.207    |                    2 |\n",
      "| train_fnn_fe339_00004 | TERMINATED | 171.67.96.198:601618 |      0.0548856 |              41 |           70 | 0.00624514  |            1 |    0.000788671 |        0.19882  |                   16 |\n",
      "| train_fnn_fe339_00005 | TERMINATED | 171.67.96.198:601617 |      0.169317  |              21 |           90 | 9.20665e-06 |            2 |    0.000112148 |       17.3654   |                    1 |\n",
      "| train_fnn_fe339_00006 | TERMINATED | 171.67.96.198:601619 |      0.637431  |              11 |           80 | 1.08423e-05 |            3 |    9.71778e-05 |       46.3597   |                    1 |\n",
      "| train_fnn_fe339_00007 | TERMINATED | 171.67.96.198:601620 |      0.401713  |              46 |           60 | 1.3343e-06  |            2 |    0.000336399 |       91.6444   |                    1 |\n",
      "| train_fnn_fe339_00008 | TERMINATED | 171.67.96.198:601621 |      0.422551  |              16 |          100 | 0.00486964  |            2 |    1.8428e-06  |        0.577009 |                    4 |\n",
      "| train_fnn_fe339_00009 | TERMINATED | 171.67.96.198:601622 |      0.382388  |               6 |           80 | 0.000222333 |            1 |    0.000786622 |        0.119962 |                   30 |\n",
      "| train_fnn_fe339_00010 | TERMINATED | 171.67.96.198:603302 |      0.0207891 |               1 |          130 | 4.93862e-05 |            1 |    1.52983e-05 |        1.90933  |                    1 |\n",
      "| train_fnn_fe339_00011 | TERMINATED | 171.67.96.198:603303 |      0.500826  |               1 |          110 | 0.00144778  |            3 |    6.57476e-05 |        0.120435 |                   30 |\n",
      "| train_fnn_fe339_00012 | TERMINATED | 171.67.96.198:603433 |      0.440076  |              41 |           90 | 2.10665e-05 |            3 |    1.55123e-06 |       43.4836   |                    1 |\n",
      "| train_fnn_fe339_00013 | TERMINATED | 171.67.96.198:603436 |      0.199558  |              36 |           50 | 0.000175782 |            3 |    1.40858e-05 |        8.08791  |                    1 |\n",
      "| train_fnn_fe339_00014 | TERMINATED | 171.67.96.198:603501 |      0.397281  |              21 |           30 | 0.00121302  |            1 |    3.02961e-05 |        0.133924 |                   30 |\n",
      "| train_fnn_fe339_00015 | TERMINATED | 171.67.96.198:603772 |      0.313142  |              46 |          150 | 6.41028e-06 |            2 |    0.000486733 |       14.2128   |                    1 |\n",
      "| train_fnn_fe339_00016 | TERMINATED | 171.67.96.198:603773 |      0.182012  |              21 |           40 | 4.38058e-05 |            2 |    0.000184779 |       10.5261   |                    1 |\n",
      "| train_fnn_fe339_00017 | TERMINATED | 171.67.96.198:604000 |      0.121243  |              11 |           70 | 0.00523348  |            3 |    0.000265681 |        1.48382  |                   16 |\n",
      "| train_fnn_fe339_00018 | TERMINATED | 171.67.96.198:604767 |      0.0942407 |              46 |           60 | 2.6619e-05  |            2 |    0.000525394 |        9.85233  |                    1 |\n",
      "| train_fnn_fe339_00019 | TERMINATED | 171.67.96.198:604768 |      0.220299  |              41 |          200 | 4.55642e-06 |            1 |    4.00192e-05 |        5.89094  |                    1 |\n",
      "| train_fnn_fe339_00020 | TERMINATED | 171.67.96.198:604945 |      0.242954  |              46 |           10 | 0.0059107   |            3 |    9.32414e-06 |        0.115363 |                   30 |\n",
      "| train_fnn_fe339_00021 | TERMINATED | 171.67.96.198:604946 |      0.185202  |               1 |           90 | 9.71637e-06 |            1 |    0.000122719 |        4.33721  |                    2 |\n",
      "| train_fnn_fe339_00022 | TERMINATED | 171.67.96.198:605068 |      0.035452  |              36 |           50 | 0.00027432  |            1 |    3.22134e-05 |        1.65457  |                    2 |\n",
      "| train_fnn_fe339_00023 | TERMINATED | 171.67.96.198:605806 |      0.175298  |               1 |           50 | 3.79821e-06 |            1 |    2.94007e-05 |        6.76045  |                    1 |\n",
      "| train_fnn_fe339_00024 | TERMINATED | 171.67.96.198:605807 |      0.17397   |              36 |           40 | 0.000818211 |            3 |    1.26867e-05 |        0.117268 |                   30 |\n",
      "| train_fnn_fe339_00025 | TERMINATED | 171.67.96.198:606225 |      0.113995  |               1 |           60 | 0.000107844 |            2 |    0.000122303 |        3.43273  |                    2 |\n",
      "| train_fnn_fe339_00026 | TERMINATED | 171.67.96.198:606226 |      0.363344  |              46 |           40 | 8.05347e-06 |            3 |    8.62022e-05 |       67.3786   |                    1 |\n",
      "| train_fnn_fe339_00027 | TERMINATED | 171.67.96.198:606291 |      0.58802   |               6 |           10 | 0.000504383 |            3 |    0.000160564 |        0.469044 |                    2 |\n",
      "| train_fnn_fe339_00028 | TERMINATED | 171.67.96.198:606912 |      0.465389  |              31 |          100 | 0.00185733  |            3 |    4.63023e-05 |        0.144056 |                   30 |\n",
      "| train_fnn_fe339_00029 | TERMINATED | 171.67.96.198:609147 |      0.631288  |              21 |           20 | 0.000340726 |            1 |    1.04013e-05 |        1.13399  |                    2 |\n",
      "| train_fnn_fe339_00030 | TERMINATED | 171.67.96.198:610530 |      0.62209   |              16 |          160 | 0.00131675  |            3 |    8.43519e-05 |        0.297692 |                    4 |\n",
      "| train_fnn_fe339_00031 | TERMINATED | 171.67.96.198:611299 |      0.428436  |              36 |           80 | 1.0884e-06  |            2 |    2.01565e-06 |       33.5616   |                    1 |\n",
      "| train_fnn_fe339_00032 | TERMINATED | 171.67.96.198:615135 |      0.345073  |               6 |          180 | 6.21866e-05 |            2 |    0.000962437 |        6.67447  |                    1 |\n",
      "| train_fnn_fe339_00033 | TERMINATED | 171.67.96.198:616500 |      0.133388  |              21 |           40 | 2.92331e-05 |            1 |    0.000170809 |        3.06844  |                    2 |\n",
      "| train_fnn_fe339_00034 | TERMINATED | 171.67.96.198:618382 |      0.44907   |              11 |           10 | 1.00426e-05 |            3 |    5.88319e-05 |       29.1474   |                    1 |\n",
      "| train_fnn_fe339_00035 | TERMINATED | 171.67.96.198:619131 |      0.445486  |              36 |          190 | 0.00151093  |            2 |    3.22041e-05 |        0.209653 |                    4 |\n",
      "| train_fnn_fe339_00036 | TERMINATED | 171.67.96.198:623397 |      0.404683  |              11 |           10 | 0.00118636  |            1 |    1.35149e-06 |        0.116714 |                   30 |\n",
      "| train_fnn_fe339_00037 | TERMINATED | 171.67.96.198:623406 |      0.26541   |              26 |          140 | 1.15299e-06 |            1 |    0.000609475 |       16.2664   |                    1 |\n",
      "| train_fnn_fe339_00038 | TERMINATED | 171.67.96.198:624755 |      0.21317   |              16 |           30 | 3.47049e-05 |            1 |    0.00035761  |        5.79955  |                    1 |\n",
      "| train_fnn_fe339_00039 | TERMINATED | 171.67.96.198:624765 |      0.655947  |              46 |          180 | 0.000608302 |            1 |    5.13078e-05 |        0.639259 |                    2 |\n",
      "+-----------------------+------------+----------------------+----------------+-----------------+--------------+-------------+--------------+----------------+-----------------+----------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger(\"ray\").setLevel(logging.WARNING) # To turn off a lot of Ray messages\n",
    "best_trial = get_best_trial(train_loader, test_loader, num_samples=40, max_num_epochs=30, gpus_per_trial=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98554816-a9a9-4f68-95ba-09cab1b10f35",
   "metadata": {},
   "source": [
    "## Retrain the model with the optimized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2bbe1e9a-357e-4321-bded-683d606b27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training Process:\n",
    "Training Years: [1980, 1981, 1982, 1983, 1984, 1985, 1986]\n",
    "Testing Year: [1987] for hyperparameters tuning\n",
    "\n",
    "Inference Process:\n",
    "Training Years: [1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987] \n",
    "using optimized hyperparameters found during training\n",
    "Inference Year: [1988]\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "64294607-04cf-499d-b4cd-bdae0907563a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_fnn_fe339_00020"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "17ff8169-6d42-4047-ba83-cccca4d7c571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_dim': 102,\n",
       " 'hidden_dim': 10,\n",
       " 'num_layers': 3,\n",
       " 'num_embeddings': 953,\n",
       " 'embedding_dim': 46,\n",
       " 'dropout_rate': 0.24295446826850328,\n",
       " 'lr': 0.005910698619088546,\n",
       " 'weight_decay': 9.324140221663508e-06,\n",
       " 'num_epochs': 30,\n",
       " 'gpu': 0}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f2892587-3dd3-4fae-aacb-8099ed00eeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neural Network:Epoch 1/30, metrics: {'avg_train_loss': 1.6898387787703253, 'avg_test_loss': 0.12120474405051057}\n",
      "INFO:Neural Network:Epoch 2/30, metrics: {'avg_train_loss': 0.10228859168162804, 'avg_test_loss': 0.12438325286885492}\n",
      "INFO:Neural Network:Epoch 3/30, metrics: {'avg_train_loss': 0.09857141108932611, 'avg_test_loss': 0.11640679848447345}\n",
      "INFO:Neural Network:Epoch 4/30, metrics: {'avg_train_loss': 0.09722370161339805, 'avg_test_loss': 0.11544685628192097}\n",
      "INFO:Neural Network:Epoch 5/30, metrics: {'avg_train_loss': 0.09622465083108306, 'avg_test_loss': 0.11625250575004839}\n",
      "INFO:Neural Network:Epoch 6/30, metrics: {'avg_train_loss': 0.0954351717766547, 'avg_test_loss': 0.11918131240150508}\n",
      "INFO:Neural Network:Epoch 7/30, metrics: {'avg_train_loss': 0.09557125286484923, 'avg_test_loss': 0.12116529415148536}\n",
      "INFO:Neural Network:Epoch 8/30, metrics: {'avg_train_loss': 0.09482452587099133, 'avg_test_loss': 0.12471790297650824}\n",
      "INFO:Neural Network:Epoch 9/30, metrics: {'avg_train_loss': 0.09506891746378235, 'avg_test_loss': 0.11891815024945471}\n",
      "INFO:Neural Network:Epoch 10/30, metrics: {'avg_train_loss': 0.09474471339721553, 'avg_test_loss': 0.11821815578473939}\n",
      "INFO:Neural Network:Epoch 11/30, metrics: {'avg_train_loss': 0.09478239865180015, 'avg_test_loss': 0.12272934868834377}\n",
      "INFO:Neural Network:Epoch 12/30, metrics: {'avg_train_loss': 0.09564731141786702, 'avg_test_loss': 0.11977898643589487}\n",
      "INFO:Neural Network:Epoch 13/30, metrics: {'avg_train_loss': 0.09471085298370578, 'avg_test_loss': 0.1249682483525058}\n",
      "INFO:Neural Network:Epoch 14/30, metrics: {'avg_train_loss': 0.0958173879962949, 'avg_test_loss': 0.11496280554949848}\n",
      "INFO:Neural Network:Epoch 15/30, metrics: {'avg_train_loss': 0.09462650714413351, 'avg_test_loss': 0.14174658659042097}\n",
      "INFO:Neural Network:Epoch 16/30, metrics: {'avg_train_loss': 0.09553894666630043, 'avg_test_loss': 0.11800939648174773}\n",
      "INFO:Neural Network:Epoch 17/30, metrics: {'avg_train_loss': 0.094903580174179, 'avg_test_loss': 0.11666689849659509}\n",
      "INFO:Neural Network:Epoch 18/30, metrics: {'avg_train_loss': 0.09481013451125185, 'avg_test_loss': 0.12119088156355752}\n",
      "INFO:Neural Network:Epoch 19/30, metrics: {'avg_train_loss': 0.09624479570432215, 'avg_test_loss': 0.11542736329868727}\n",
      "INFO:Neural Network:Epoch 20/30, metrics: {'avg_train_loss': 0.0956312520450051, 'avg_test_loss': 0.27710945176240664}\n",
      "INFO:Neural Network:Epoch 21/30, metrics: {'avg_train_loss': 0.09543727094073169, 'avg_test_loss': 0.11630654091538947}\n",
      "INFO:Neural Network:Epoch 22/30, metrics: {'avg_train_loss': 0.0948752471210875, 'avg_test_loss': 0.11636046232546077}\n",
      "INFO:Neural Network:Epoch 23/30, metrics: {'avg_train_loss': 0.0958526534075637, 'avg_test_loss': 0.1225662058572364}\n",
      "INFO:Neural Network:Epoch 24/30, metrics: {'avg_train_loss': 0.09476207715621585, 'avg_test_loss': 0.15820757114614536}\n",
      "INFO:Neural Network:Epoch 25/30, metrics: {'avg_train_loss': 0.0958692305916653, 'avg_test_loss': 0.12365397282577807}\n",
      "INFO:Neural Network:Epoch 26/30, metrics: {'avg_train_loss': 0.09495061650773548, 'avg_test_loss': 0.12057184242929508}\n",
      "INFO:Neural Network:Epoch 27/30, metrics: {'avg_train_loss': 0.09494992211526856, 'avg_test_loss': 0.11597005100129476}\n",
      "INFO:Neural Network:Epoch 28/30, metrics: {'avg_train_loss': 0.09840542156142378, 'avg_test_loss': 0.11797675845864551}\n",
      "INFO:Neural Network:Epoch 29/30, metrics: {'avg_train_loss': 0.09451040929434183, 'avg_test_loss': 0.11742175462978338}\n",
      "INFO:Neural Network:Epoch 30/30, metrics: {'avg_train_loss': 0.09552857621468061, 'avg_test_loss': 0.11770257456045524}\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_fnn(\n",
    "    config=best_trial.config, \n",
    "    train_loader=train_loader, \n",
    "    test_loader=test_loader, \n",
    "    ray_tuning=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d37d4c50-71c7-44bb-b75a-f4f5ca5c3231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0232391 , -0.0139509 , -0.02017702, ..., -0.02177294,\n",
       "       -0.03343377, -0.02854417], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80509bfa-b934-41d9-bb34-6ff9520d3e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wolee_edehaan_suzienoh_exploratory-ml",
   "language": "python",
   "name": "wolee_edehaan_suzienoh_exploratory-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
